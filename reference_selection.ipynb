{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üéØ c Selection Tool\n",
    "\n",
    "Bu notebook ile validation analysis sonu√ßlarƒ±ndan **en iyi reference solutions**'ƒ± se√ßeceƒüiz:\n",
    "1. Validation results'ƒ± y√ºkleyeceƒüiz\n",
    "2. Her kategori i√ßin top candidates'ƒ± inceleyeceƒüiz  \n",
    "3. Manual review ile final **Musterl√∂sung** se√ßeceƒüiz\n",
    "4. Reference database olu≈üturacaƒüƒ±z\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîß Setup ve Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Results directory: results/validation_analysis/\n",
      "üìÑ Available result files: 1\n",
      "üéØ Available candidate files: 1\n",
      "\n",
      "üìä Latest results: validation_claude_results_20250619_191555.json\n",
      "üéØ Latest candidates: reference_candidates_claude_20250619_191555.json\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "RESULTS_DIR = \"results/validation_analysis/\"\n",
    "VAL_DIR = r\"C:\\Users\\egese\\Desktop\\dataset\\val\\SAP\"\n",
    "REFERENCE_OUTPUT = \"results/reference_solutions/\"\n",
    "\n",
    "# Output directory olu≈ütur\n",
    "os.makedirs(REFERENCE_OUTPUT, exist_ok=True)\n",
    "\n",
    "# Available result files listele\n",
    "if os.path.exists(RESULTS_DIR):\n",
    "    result_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith('validation_claude_results_')]\n",
    "    candidate_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith('reference_candidates_')]\n",
    "    \n",
    "    print(f\"üìÅ Results directory: {RESULTS_DIR}\")\n",
    "    print(f\"üìÑ Available result files: {len(result_files)}\")\n",
    "    print(f\"üéØ Available candidate files: {len(candidate_files)}\")\n",
    "    \n",
    "    if result_files:\n",
    "        latest_results = sorted(result_files)[-1]\n",
    "        print(f\"\\nüìä Latest results: {latest_results}\")\n",
    "    \n",
    "    if candidate_files:\n",
    "        latest_candidates = sorted(candidate_files)[-1]\n",
    "        print(f\"üéØ Latest candidates: {latest_candidates}\")\n",
    "else:\n",
    "    print(\"‚ùå Results directory not found! Run validation analysis first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading: results/validation_analysis/validation_claude_results_20250619_191555.json\n",
      "‚úÖ Loaded validation results: 10 images\n",
      "üéØ Loaded reference candidates: 3 categories\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Load validation results\n",
    "validation_results = []\n",
    "reference_candidates = {}\n",
    "\n",
    "if 'latest_results' in locals():\n",
    "    results_path = os.path.join(RESULTS_DIR, latest_results)\n",
    "    print(f\"üìÑ Loading: {results_path}\")\n",
    "    \n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        validation_results = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded validation results: {len(validation_results)} images\")\n",
    "    \n",
    "    # Load candidates if available\n",
    "    if 'latest_candidates' in locals():\n",
    "        candidates_path = os.path.join(RESULTS_DIR, latest_candidates)\n",
    "        with open(candidates_path, 'r', encoding='utf-8') as f:\n",
    "            reference_candidates = json.load(f)\n",
    "        \n",
    "        print(f\"üéØ Loaded reference candidates: {len(reference_candidates)} categories\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No candidate file found, will generate from results\")\n",
    "else:\n",
    "    print(\"‚ùå No validation results found!\")\n",
    "    print(\"üí° Please run validation_claude_analysis.ipynb first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ANALYZING 10 VALIDATION RESULTS\n",
      "‚úÖ Successful: 10\n",
      "‚ùå Failed: 0\n",
      "\n",
      "üìä CATEGORY ANALYSIS FOR REFERENCE SELECTION\n",
      "============================================================\n",
      "\n",
      "üìÅ Data Source:\n",
      "   Total Images: 2\n",
      "   Avg CNN Confidence: 71.20%\n",
      "   Avg Claude Score: 0.0/10\n",
      "   Claude Success Rate: 0.0%\n",
      "   Top candidates available: 2\n",
      "\n",
      "üìÅ Excel-Tabelle:\n",
      "   Total Images: 2\n",
      "   Avg CNN Confidence: 99.23%\n",
      "   Avg Claude Score: 8.0/10\n",
      "   Claude Success Rate: 100.0%\n",
      "   Top candidates available: 2\n",
      "\n",
      "üìÅ Info-Object:\n",
      "   Total Images: 6\n",
      "   Avg CNN Confidence: 67.34%\n",
      "   Avg Claude Score: 7.0/10\n",
      "   Claude Success Rate: 33.3%\n",
      "   Top candidates available: 5\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Analyze validation results and create selection tool\n",
    "def analyze_validation_results(results):\n",
    "    \"\"\"Validation results'ƒ±n detaylƒ± analizi\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå No validation results to analyze!\")\n",
    "        return {}\n",
    "    \n",
    "    successful_results = [r for r in results if r.get('status') == 'success']\n",
    "    \n",
    "    print(f\"üìä ANALYZING {len(results)} VALIDATION RESULTS\")\n",
    "    print(f\"‚úÖ Successful: {len(successful_results)}\")\n",
    "    print(f\"‚ùå Failed: {len(results) - len(successful_results)}\")\n",
    "    \n",
    "    # Category statistics\n",
    "    category_stats = {}\n",
    "    \n",
    "    for result in successful_results:\n",
    "        category = result['cnn_prediction']\n",
    "        if category not in category_stats:\n",
    "            category_stats[category] = {\n",
    "                'count': 0,\n",
    "                'items': []\n",
    "            }\n",
    "        \n",
    "        category_stats[category]['count'] += 1\n",
    "        \n",
    "        # CNN confidence\n",
    "        cnn_conf = result['cnn_confidence']\n",
    "        \n",
    "        # Claude analysis\n",
    "        claude_analysis = result.get('claude_analysis', {})\n",
    "        claude_score = 0\n",
    "        claude_success = False\n",
    "        \n",
    "        if 'error' not in claude_analysis and 'gesamt_score' in claude_analysis:\n",
    "            claude_score = claude_analysis['gesamt_score']\n",
    "            claude_success = True\n",
    "        \n",
    "        # Combined score for ranking\n",
    "        combined_score = (cnn_conf * 0.3) + (claude_score/10 * 0.7) if claude_success else cnn_conf * 0.5\n",
    "        \n",
    "        category_stats[category]['items'].append({\n",
    "            'filename': result['filename'],\n",
    "            'cnn_confidence': cnn_conf,\n",
    "            'claude_score': claude_score,\n",
    "            'claude_success': claude_success,\n",
    "            'combined_score': combined_score,\n",
    "            'result': result\n",
    "        })\n",
    "    \n",
    "    # Sort items by combined score\n",
    "    for category, stats in category_stats.items():\n",
    "        stats['items'] = sorted(stats['items'], key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        # Calculate averages\n",
    "        if stats['count'] > 0:\n",
    "            stats['avg_cnn_confidence'] = sum(item['cnn_confidence'] for item in stats['items']) / stats['count']\n",
    "            \n",
    "            claude_successful = [item for item in stats['items'] if item['claude_success']]\n",
    "            stats['claude_success_rate'] = len(claude_successful) / stats['count'] * 100\n",
    "            \n",
    "            if claude_successful:\n",
    "                stats['avg_claude_score'] = sum(item['claude_score'] for item in claude_successful) / len(claude_successful)\n",
    "            else:\n",
    "                stats['avg_claude_score'] = 0\n",
    "    \n",
    "    return category_stats\n",
    "\n",
    "# Analyze results\n",
    "category_analysis = analyze_validation_results(validation_results)\n",
    "\n",
    "if category_analysis:\n",
    "    print(\"\\nüìä CATEGORY ANALYSIS FOR REFERENCE SELECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for category, stats in sorted(category_analysis.items()):\n",
    "        print(f\"\\nüìÅ {category}:\")\n",
    "        print(f\"   Total Images: {stats['count']}\")\n",
    "        print(f\"   Avg CNN Confidence: {stats['avg_cnn_confidence']:.2%}\")\n",
    "        print(f\"   Avg Claude Score: {stats['avg_claude_score']:.1f}/10\")\n",
    "        print(f\"   Claude Success Rate: {stats['claude_success_rate']:.1f}%\")\n",
    "        print(f\"   Top candidates available: {min(5, len(stats['items']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MANUAL REFERENCE SELECTION\n",
      "========================================\n",
      "üìã Available categories:\n",
      "   1. Info-Object (6 images)\n",
      "   2. Excel-Tabelle (2 images)\n",
      "   3. Data Source (2 images)\n",
      "\n",
      "üîç TO VIEW CANDIDATES FOR A CATEGORY:\n",
      "# View top 3 candidates for Info-Object\n",
      "if 'Info-Object' in category_analysis:\n",
      "    stats = category_analysis['Info-Object']\n",
      "    candidates = stats['items'][:3]\n",
      "    for i, candidate in enumerate(candidates, 1):\n",
      "        print(f'{i}. {candidate[\"filename\"]} - CNN: {candidate[\"cnn_confidence\"]:.2%} - Claude: {candidate[\"claude_score\"]:.1f}/10')\n",
      "\n",
      "üí° TO SELECT A REFERENCE:\n",
      "# Add your manual selections here:\n",
      "selected_references = {\n",
      "    # 'Info-Object': 'your_chosen_filename.jpg',\n",
      "    # 'Excel-Tabelle': 'your_chosen_filename.jpg',\n",
      "    # 'Data Source': 'your_chosen_filename.jpg',\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Manual selection commands\n",
    "if category_analysis:\n",
    "    print(\"üéØ MANUAL REFERENCE SELECTION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    categories = list(category_analysis.keys())\n",
    "    print(\"üìã Available categories:\")\n",
    "    for i, cat in enumerate(categories, 1):\n",
    "        count = category_analysis[cat]['count']\n",
    "        print(f\"   {i}. {cat} ({count} images)\")\n",
    "    \n",
    "    print(\"\\nüîç TO VIEW CANDIDATES FOR A CATEGORY:\")\n",
    "    if categories:\n",
    "        example_cat = categories[0]\n",
    "        print(f\"# View top 3 candidates for {example_cat}\")\n",
    "        print(f\"if '{example_cat}' in category_analysis:\")\n",
    "        print(f\"    stats = category_analysis['{example_cat}']\")\n",
    "        print(f\"    candidates = stats['items'][:3]\")\n",
    "        print(f\"    for i, candidate in enumerate(candidates, 1):\")\n",
    "        print(f\"        print(f'{{i}}. {{candidate[\\\"filename\\\"]}} - CNN: {{candidate[\\\"cnn_confidence\\\"]:.2%}} - Claude: {{candidate[\\\"claude_score\\\"]:.1f}}/10')\")\n",
    "    \n",
    "    print(\"\\nüí° TO SELECT A REFERENCE:\")\n",
    "    print(\"# Add your manual selections here:\")\n",
    "    print(\"selected_references = {\")\n",
    "    for cat in categories:\n",
    "        print(f\"    # '{cat}': 'your_chosen_filename.jpg',\")\n",
    "    print(\"}\")\n",
    "else:\n",
    "    print(\"‚ùå No categories available for selection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç QUICK VIEW: Info-Object\n",
      "========================================\n",
      "\n",
      "1. 005a2c943c5a4344bc058a3680a62c43.png\n",
      "   üìä CNN Confidence: 96.58%\n",
      "   ü§ñ Claude Score: 8.0/10\n",
      "   ‚≠ê Combined Score: 0.850\n",
      "\n",
      "2. 0056d6ebd8c045ea8bac5910b8aee061.jpeg\n",
      "   üìä CNN Confidence: 89.97%\n",
      "   ü§ñ Claude Score: 6.0/10\n",
      "   ‚≠ê Combined Score: 0.690\n",
      "\n",
      "3. 001e53127c8049dc94ead93b884b92fa.jpeg\n",
      "   üìä CNN Confidence: 91.61%\n",
      "   ü§ñ Claude Score: ‚ùå Failed\n",
      "   ‚≠ê Combined Score: 0.458\n",
      "\n",
      "üí° To select the best one, you can manually choose:\n",
      "# Example: selected_references = {'Info-Object': '005a2c943c5a4344bc058a3680a62c43.png'}\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Quick test - show first category candidates\n",
    "if category_analysis:\n",
    "    first_category = list(category_analysis.keys())[0]\n",
    "    print(f\"üîç QUICK VIEW: {first_category}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    stats = category_analysis[first_category]\n",
    "    candidates = stats['items'][:3]\n",
    "    \n",
    "    for i, candidate in enumerate(candidates, 1):\n",
    "        print(f\"\\n{i}. {candidate['filename']}\")\n",
    "        print(f\"   üìä CNN Confidence: {candidate['cnn_confidence']:.2%}\")\n",
    "        if candidate['claude_success']:\n",
    "            print(f\"   ü§ñ Claude Score: {candidate['claude_score']:.1f}/10\")\n",
    "        else:\n",
    "            print(f\"   ü§ñ Claude Score: ‚ùå Failed\")\n",
    "        print(f\"   ‚≠ê Combined Score: {candidate['combined_score']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüí° To select the best one, you can manually choose:\")\n",
    "    print(f\"# Example: selected_references = {{'{first_category}': '{candidates[0]['filename']}'}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AUTOMATIC REFERENCE SELECTION ENGINE\n",
      "==================================================\n",
      "üìä Processing 10 successful results\n",
      "\n",
      "üéØ AUTOMATIC SELECTION RESULTS:\n",
      "--------------------------------------------------\n",
      "‚úÖ Info-Object:\n",
      "   üìÑ File: 005a2c943c5a4344bc058a3680a62c43.png\n",
      "   üìä CNN: 96.58%\n",
      "   ü§ñ Claude: 8.0/10\n",
      "   ‚≠ê Score: 0.850\n",
      "\n",
      "‚úÖ Excel-Tabelle:\n",
      "   üìÑ File: 0042c7e49bb143ca9b50b3d6336c003b.png\n",
      "   üìä CNN: 99.86%\n",
      "   ü§ñ Claude: 8.0/10\n",
      "   ‚≠ê Score: 0.860\n",
      "\n",
      "‚úÖ Data Source:\n",
      "   üìÑ File: 008e8269e68341848a3680ebc4f11910.jpeg\n",
      "   üìä CNN: 99.82%\n",
      "   ü§ñ Claude: ‚ùå Failed\n",
      "   ‚≠ê Score: 0.499\n",
      "\n",
      "üéâ AUTOMATIC SELECTION COMPLETE!\n",
      "üìÅ Selected references for 3 categories\n"
     ]
    }
   ],
   "source": [
    "def automatic_reference_selection(validation_results, min_score_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Tamamen otomatik reference selection\n",
    "    En y√ºksek combined score'a g√∂re her kategoriden en iyi adaylarƒ± se√ßer\n",
    "    \"\"\"\n",
    "    \n",
    "    if not validation_results:\n",
    "        print(\"‚ùå No validation results!\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"ü§ñ AUTOMATIC REFERENCE SELECTION ENGINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Successful results only\n",
    "    successful_results = [r for r in validation_results if r.get('status') == 'success']\n",
    "    print(f\"üìä Processing {len(successful_results)} successful results\")\n",
    "    \n",
    "    # Group by category\n",
    "    categories = {}\n",
    "    \n",
    "    for result in successful_results:\n",
    "        category = result['cnn_prediction']\n",
    "        if category not in categories:\n",
    "            categories[category] = []\n",
    "        \n",
    "        # Calculate scores\n",
    "        cnn_conf = result['cnn_confidence']\n",
    "        claude_analysis = result.get('claude_analysis', {})\n",
    "        \n",
    "        # Claude score\n",
    "        claude_score = 0\n",
    "        claude_success = False\n",
    "        if 'error' not in claude_analysis and 'gesamt_score' in claude_analysis:\n",
    "            claude_score = claude_analysis['gesamt_score']\n",
    "            claude_success = True\n",
    "        \n",
    "        # Combined score calculation\n",
    "        if claude_success:\n",
    "            combined_score = (cnn_conf * 0.3) + (claude_score/10 * 0.7)  # Claude weighted more\n",
    "        else:\n",
    "            combined_score = cnn_conf * 0.5  # Penalty for Claude failure\n",
    "        \n",
    "        candidate = {\n",
    "            'filename': result['filename'],\n",
    "            'cnn_confidence': cnn_conf,\n",
    "            'claude_score': claude_score,\n",
    "            'claude_success': claude_success,\n",
    "            'combined_score': combined_score,\n",
    "            'full_result': result\n",
    "        }\n",
    "        \n",
    "        categories[category].append(candidate)\n",
    "    \n",
    "    # Select best reference for each category\n",
    "    selected_references = {}\n",
    "    \n",
    "    print(f\"\\nüéØ AUTOMATIC SELECTION RESULTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for category, candidates in categories.items():\n",
    "        # Sort by combined score (highest first)\n",
    "        sorted_candidates = sorted(candidates, key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        # Get best candidate\n",
    "        best_candidate = sorted_candidates[0]\n",
    "        \n",
    "        # Quality check\n",
    "        if best_candidate['combined_score'] >= min_score_threshold:\n",
    "            selected_references[category] = {\n",
    "                'filename': best_candidate['filename'],\n",
    "                'selection_reason': f\"Highest combined score: {best_candidate['combined_score']:.3f}\",\n",
    "                'cnn_confidence': best_candidate['cnn_confidence'],\n",
    "                'claude_score': best_candidate['claude_score'],\n",
    "                'claude_success': best_candidate['claude_success'],\n",
    "                'combined_score': best_candidate['combined_score'],\n",
    "                'selected_at': datetime.now().isoformat(),\n",
    "                'selection_method': 'automatic_best_score'\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ {category}:\")\n",
    "            print(f\"   üìÑ File: {best_candidate['filename']}\")\n",
    "            print(f\"   üìä CNN: {best_candidate['cnn_confidence']:.2%}\")\n",
    "            if best_candidate['claude_success']:\n",
    "                print(f\"   ü§ñ Claude: {best_candidate['claude_score']:.1f}/10\")\n",
    "            else:\n",
    "                print(f\"   ü§ñ Claude: ‚ùå Failed\")\n",
    "            print(f\"   ‚≠ê Score: {best_candidate['combined_score']:.3f}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {category}: Best score {best_candidate['combined_score']:.3f} below threshold {min_score_threshold}\")\n",
    "    \n",
    "    return selected_references\n",
    "\n",
    "# Execute automatic selection\n",
    "automatic_references = automatic_reference_selection(validation_results)\n",
    "\n",
    "print(f\"üéâ AUTOMATIC SELECTION COMPLETE!\")\n",
    "print(f\"üìÅ Selected references for {len(automatic_references)} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AUTOMATIC REFERENCES SAVED!\n",
      "üìÑ File: reference_solutions_automatic_20250619_193609.json\n",
      "üìä Categories: 3\n",
      "üìã Summary: automatic_selection_summary_20250619_193609.txt\n",
      "\n",
      "üìà SELECTION STATISTICS:\n",
      "   üéØ Categories processed: 3\n",
      "   üìä Total candidates considered: 10\n",
      "   ‚≠ê Average combined score: 0.736\n",
      "   ü§ñ Claude success rate: 66.7%\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   1Ô∏è‚É£ References are ready for comparison_engine.ipynb\n",
      "   2Ô∏è‚É£ Run aufgabe4_final_evaluation.ipynb for testing\n",
      "   3Ô∏è‚É£ Reference file: reference_solutions_automatic_20250619_193609.json\n"
     ]
    }
   ],
   "source": [
    "# Save automatic reference selections\n",
    "if automatic_references:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"reference_solutions_automatic_{timestamp}.json\"\n",
    "    output_path = os.path.join(REFERENCE_OUTPUT, output_filename)\n",
    "    \n",
    "    # Prepare final reference data\n",
    "    reference_data = {\n",
    "        'created_at': timestamp,\n",
    "        'selection_method': 'automatic_best_score',\n",
    "        'total_categories': len(automatic_references),\n",
    "        'total_references': len(automatic_references),\n",
    "        'selection_criteria': {\n",
    "            'cnn_weight': 0.3,\n",
    "            'claude_weight': 0.7,\n",
    "            'min_threshold': 0.4,\n",
    "            'penalty_for_claude_failure': 0.5\n",
    "        },\n",
    "        'references': {\n",
    "            category: [selection] for category, selection in automatic_references.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(reference_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ AUTOMATIC REFERENCES SAVED!\")\n",
    "    print(f\"üìÑ File: {output_filename}\")\n",
    "    print(f\"üìä Categories: {len(automatic_references)}\")\n",
    "    \n",
    "    # Create summary file\n",
    "    summary_filename = f\"automatic_selection_summary_{timestamp}.txt\"\n",
    "    summary_path = os.path.join(REFERENCE_OUTPUT, summary_filename)\n",
    "    \n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"AUTOMATIC REFERENCE SELECTION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Selection Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Selection Method: Automatic (Best Combined Score)\\n\")\n",
    "        f.write(f\"Total Categories: {len(automatic_references)}\\n\\n\")\n",
    "        \n",
    "        for category, selection in automatic_references.items():\n",
    "            f.write(f\"üìÅ {category}:\\n\")\n",
    "            f.write(f\"   ‚úÖ {selection['filename']}\\n\")\n",
    "            f.write(f\"   üìä CNN: {selection['cnn_confidence']:.2%}\\n\")\n",
    "            if selection['claude_success']:\n",
    "                f.write(f\"   ü§ñ Claude: {selection['claude_score']:.1f}/10\\n\")\n",
    "            else:\n",
    "                f.write(f\"   ü§ñ Claude: ‚ùå Failed\\n\")\n",
    "            f.write(f\"   ‚≠ê Combined Score: {selection['combined_score']:.3f}\\n\")\n",
    "            f.write(f\"   üìù Reason: {selection['selection_reason']}\\n\\n\")\n",
    "    \n",
    "    print(f\"üìã Summary: {summary_filename}\")\n",
    "    \n",
    "    # Show final statistics\n",
    "    total_candidates = sum(len(category_analysis[cat]['items']) for cat in automatic_references.keys())\n",
    "    avg_score = sum(ref['combined_score'] for ref in automatic_references.values()) / len(automatic_references)\n",
    "    claude_success_rate = sum(1 for ref in automatic_references.values() if ref['claude_success']) / len(automatic_references) * 100\n",
    "    \n",
    "    print(f\"\\nüìà SELECTION STATISTICS:\")\n",
    "    print(f\"   üéØ Categories processed: {len(automatic_references)}\")\n",
    "    print(f\"   üìä Total candidates considered: {total_candidates}\")\n",
    "    print(f\"   ‚≠ê Average combined score: {avg_score:.3f}\")\n",
    "    print(f\"   ü§ñ Claude success rate: {claude_success_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "    print(f\"   1Ô∏è‚É£ References are ready for comparison_engine.ipynb\")\n",
    "    print(f\"   2Ô∏è‚É£ Run aufgabe4_final_evaluation.ipynb for testing\")\n",
    "    print(f\"   3Ô∏è‚É£ Reference file: {output_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No references were automatically selected!\")\n",
    "    print(\"üí° Check validation results and try lowering the threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç QUALITY VERIFICATION OF AUTOMATIC SELECTIONS\n",
      "==================================================\n",
      "\n",
      "üìÅ Info-Object:\n",
      "   üìÑ Selected: 005a2c943c5a4344bc058a3680a62c43.png\n",
      "   ‚úÖ File exists (34.3 KB)\n",
      "   üìä Quality scores:\n",
      "      CNN Confidence: 96.58%\n",
      "      Claude Score: 8.0/10\n",
      "      Combined Score: 0.850\n",
      "\n",
      "üìÅ Excel-Tabelle:\n",
      "   üìÑ Selected: 0042c7e49bb143ca9b50b3d6336c003b.png\n",
      "   ‚úÖ File exists (254.1 KB)\n",
      "   üìä Quality scores:\n",
      "      CNN Confidence: 99.86%\n",
      "      Claude Score: 8.0/10\n",
      "      Combined Score: 0.860\n",
      "\n",
      "üìÅ Data Source:\n",
      "   üìÑ Selected: 008e8269e68341848a3680ebc4f11910.jpeg\n",
      "   ‚úÖ File exists (88.9 KB)\n",
      "   üìä Quality scores:\n",
      "      CNN Confidence: 99.82%\n",
      "      Claude Score: ‚ùå Failed (using CNN only)\n",
      "      Combined Score: 0.499 (penalized)\n",
      "\n",
      "üéâ AUTOMATIC REFERENCE SELECTION COMPLETED!\n",
      "‚úÖ Ready for Aufgabe 4 comparison engine!\n"
     ]
    }
   ],
   "source": [
    "# Verify automatic selections\n",
    "if automatic_references:\n",
    "    print(\"üîç QUALITY VERIFICATION OF AUTOMATIC SELECTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, selection in automatic_references.items():\n",
    "        print(f\"\\nüìÅ {category}:\")\n",
    "        print(f\"   üìÑ Selected: {selection['filename']}\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        file_path = os.path.join(VAL_DIR, selection['filename'])\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path) / 1024\n",
    "            print(f\"   ‚úÖ File exists ({file_size:.1f} KB)\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå File not found!\")\n",
    "        \n",
    "        # Quality scores\n",
    "        print(f\"   üìä Quality scores:\")\n",
    "        print(f\"      CNN Confidence: {selection['cnn_confidence']:.2%}\")\n",
    "        if selection['claude_success']:\n",
    "            print(f\"      Claude Score: {selection['claude_score']:.1f}/10\")\n",
    "            print(f\"      Combined Score: {selection['combined_score']:.3f}\")\n",
    "        else:\n",
    "            print(f\"      Claude Score: ‚ùå Failed (using CNN only)\")\n",
    "            print(f\"      Combined Score: {selection['combined_score']:.3f} (penalized)\")\n",
    "    \n",
    "    print(f\"\\nüéâ AUTOMATIC REFERENCE SELECTION COMPLETED!\")\n",
    "    print(f\"‚úÖ Ready for Aufgabe 4 comparison engine!\")\n",
    "else:\n",
    "    print(\"‚ùå No automatic selections available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
