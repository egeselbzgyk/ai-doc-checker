{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ğŸ¯ c Selection Tool\n",
    "\n",
    "Bu notebook ile validation analysis sonuÃ§larÄ±ndan **en iyi reference solutions**'Ä± seÃ§eceÄŸiz:\n",
    "1. Validation results'Ä± yÃ¼kleyeceÄŸiz\n",
    "2. Her kategori iÃ§in top candidates'Ä± inceleyeceÄŸiz  \n",
    "3. Manual review ile final **MusterlÃ¶sung** seÃ§eceÄŸiz\n",
    "4. Reference database oluÅŸturacaÄŸÄ±z\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ”§ Setup ve Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Results directory: results/validation_analysis/\n",
      "ğŸ“„ Available result files: 1\n",
      "ğŸ¯ Available candidate files: 1\n",
      "\n",
      "ğŸ“Š Latest results: validation_claude_results_20250619_191555.json\n",
      "ğŸ¯ Latest candidates: reference_candidates_claude_20250619_191555.json\n"
     ]
    }
   ],
   "source": [
    "# Data paths\n",
    "RESULTS_DIR = \"results/validation_analysis/\"\n",
    "VAL_DIR = r\"C:\\Users\\egese\\Desktop\\dataset\\val\\SAP\"\n",
    "REFERENCE_OUTPUT = \"results/reference_solutions/\"\n",
    "\n",
    "# Output directory oluÅŸtur\n",
    "os.makedirs(REFERENCE_OUTPUT, exist_ok=True)\n",
    "\n",
    "# Available result files listele\n",
    "if os.path.exists(RESULTS_DIR):\n",
    "    result_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith('validation_claude_results_')]\n",
    "    candidate_files = [f for f in os.listdir(RESULTS_DIR) if f.startswith('reference_candidates_')]\n",
    "    \n",
    "    print(f\"ğŸ“ Results directory: {RESULTS_DIR}\")\n",
    "    print(f\"ğŸ“„ Available result files: {len(result_files)}\")\n",
    "    print(f\"ğŸ¯ Available candidate files: {len(candidate_files)}\")\n",
    "    \n",
    "    if result_files:\n",
    "        latest_results = sorted(result_files)[-1]\n",
    "        print(f\"\\nğŸ“Š Latest results: {latest_results}\")\n",
    "    \n",
    "    if candidate_files:\n",
    "        latest_candidates = sorted(candidate_files)[-1]\n",
    "        print(f\"ğŸ¯ Latest candidates: {latest_candidates}\")\n",
    "else:\n",
    "    print(\"âŒ Results directory not found! Run validation analysis first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loading: results/validation_analysis/validation_claude_results_20250619_191555.json\n",
      "âœ… Loaded validation results: 10 images\n",
      "ğŸ¯ Loaded reference candidates: 3 categories\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Load validation results\n",
    "validation_results = []\n",
    "reference_candidates = {}\n",
    "\n",
    "if 'latest_results' in locals():\n",
    "    results_path = os.path.join(RESULTS_DIR, latest_results)\n",
    "    print(f\"ğŸ“„ Loading: {results_path}\")\n",
    "    \n",
    "    with open(results_path, 'r', encoding='utf-8') as f:\n",
    "        validation_results = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… Loaded validation results: {len(validation_results)} images\")\n",
    "    \n",
    "    # Load candidates if available\n",
    "    if 'latest_candidates' in locals():\n",
    "        candidates_path = os.path.join(RESULTS_DIR, latest_candidates)\n",
    "        with open(candidates_path, 'r', encoding='utf-8') as f:\n",
    "            reference_candidates = json.load(f)\n",
    "        \n",
    "        print(f\"ğŸ¯ Loaded reference candidates: {len(reference_candidates)} categories\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No candidate file found, will generate from results\")\n",
    "else:\n",
    "    print(\"âŒ No validation results found!\")\n",
    "    print(\"ğŸ’¡ Please run validation_claude_analysis.ipynb first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ANALYZING 10 VALIDATION RESULTS\n",
      "âœ… Successful: 10\n",
      "âŒ Failed: 0\n",
      "\n",
      "ğŸ“Š CATEGORY ANALYSIS FOR REFERENCE SELECTION\n",
      "============================================================\n",
      "\n",
      "ğŸ“ Data Source:\n",
      "   Total Images: 2\n",
      "   Avg CNN Confidence: 71.20%\n",
      "   Avg Claude Score: 0.0/10\n",
      "   Claude Success Rate: 0.0%\n",
      "   Top candidates available: 2\n",
      "\n",
      "ğŸ“ Excel-Tabelle:\n",
      "   Total Images: 2\n",
      "   Avg CNN Confidence: 99.23%\n",
      "   Avg Claude Score: 8.0/10\n",
      "   Claude Success Rate: 100.0%\n",
      "   Top candidates available: 2\n",
      "\n",
      "ğŸ“ Info-Object:\n",
      "   Total Images: 6\n",
      "   Avg CNN Confidence: 67.34%\n",
      "   Avg Claude Score: 7.0/10\n",
      "   Claude Success Rate: 33.3%\n",
      "   Top candidates available: 5\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Analyze validation results and create selection tool\n",
    "def analyze_validation_results(results):\n",
    "    \"\"\"Validation results'Ä±n detaylÄ± analizi\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"âŒ No validation results to analyze!\")\n",
    "        return {}\n",
    "    \n",
    "    successful_results = [r for r in results if r.get('status') == 'success']\n",
    "    \n",
    "    print(f\"ğŸ“Š ANALYZING {len(results)} VALIDATION RESULTS\")\n",
    "    print(f\"âœ… Successful: {len(successful_results)}\")\n",
    "    print(f\"âŒ Failed: {len(results) - len(successful_results)}\")\n",
    "    \n",
    "    # Category statistics\n",
    "    category_stats = {}\n",
    "    \n",
    "    for result in successful_results:\n",
    "        category = result['cnn_prediction']\n",
    "        if category not in category_stats:\n",
    "            category_stats[category] = {\n",
    "                'count': 0,\n",
    "                'items': []\n",
    "            }\n",
    "        \n",
    "        category_stats[category]['count'] += 1\n",
    "        \n",
    "        # CNN confidence\n",
    "        cnn_conf = result['cnn_confidence']\n",
    "        \n",
    "        # Claude analysis\n",
    "        claude_analysis = result.get('claude_analysis', {})\n",
    "        claude_score = 0\n",
    "        claude_success = False\n",
    "        \n",
    "        if 'error' not in claude_analysis and 'gesamt_score' in claude_analysis:\n",
    "            claude_score = claude_analysis['gesamt_score']\n",
    "            claude_success = True\n",
    "        \n",
    "        # Combined score for ranking\n",
    "        combined_score = (cnn_conf * 0.3) + (claude_score/10 * 0.7) if claude_success else cnn_conf * 0.5\n",
    "        \n",
    "        category_stats[category]['items'].append({\n",
    "            'filename': result['filename'],\n",
    "            'cnn_confidence': cnn_conf,\n",
    "            'claude_score': claude_score,\n",
    "            'claude_success': claude_success,\n",
    "            'combined_score': combined_score,\n",
    "            'result': result\n",
    "        })\n",
    "    \n",
    "    # Sort items by combined score\n",
    "    for category, stats in category_stats.items():\n",
    "        stats['items'] = sorted(stats['items'], key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        # Calculate averages\n",
    "        if stats['count'] > 0:\n",
    "            stats['avg_cnn_confidence'] = sum(item['cnn_confidence'] for item in stats['items']) / stats['count']\n",
    "            \n",
    "            claude_successful = [item for item in stats['items'] if item['claude_success']]\n",
    "            stats['claude_success_rate'] = len(claude_successful) / stats['count'] * 100\n",
    "            \n",
    "            if claude_successful:\n",
    "                stats['avg_claude_score'] = sum(item['claude_score'] for item in claude_successful) / len(claude_successful)\n",
    "            else:\n",
    "                stats['avg_claude_score'] = 0\n",
    "    \n",
    "    return category_stats\n",
    "\n",
    "# Analyze results\n",
    "category_analysis = analyze_validation_results(validation_results)\n",
    "\n",
    "if category_analysis:\n",
    "    print(\"\\nğŸ“Š CATEGORY ANALYSIS FOR REFERENCE SELECTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for category, stats in sorted(category_analysis.items()):\n",
    "        print(f\"\\nğŸ“ {category}:\")\n",
    "        print(f\"   Total Images: {stats['count']}\")\n",
    "        print(f\"   Avg CNN Confidence: {stats['avg_cnn_confidence']:.2%}\")\n",
    "        print(f\"   Avg Claude Score: {stats['avg_claude_score']:.1f}/10\")\n",
    "        print(f\"   Claude Success Rate: {stats['claude_success_rate']:.1f}%\")\n",
    "        print(f\"   Top candidates available: {min(5, len(stats['items']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ MANUAL REFERENCE SELECTION\n",
      "========================================\n",
      "ğŸ“‹ Available categories:\n",
      "   1. Info-Object (6 images)\n",
      "   2. Excel-Tabelle (2 images)\n",
      "   3. Data Source (2 images)\n",
      "\n",
      "ğŸ” TO VIEW CANDIDATES FOR A CATEGORY:\n",
      "# View top 3 candidates for Info-Object\n",
      "if 'Info-Object' in category_analysis:\n",
      "    stats = category_analysis['Info-Object']\n",
      "    candidates = stats['items'][:3]\n",
      "    for i, candidate in enumerate(candidates, 1):\n",
      "        print(f'{i}. {candidate[\"filename\"]} - CNN: {candidate[\"cnn_confidence\"]:.2%} - Claude: {candidate[\"claude_score\"]:.1f}/10')\n",
      "\n",
      "ğŸ’¡ TO SELECT A REFERENCE:\n",
      "# Add your manual selections here:\n",
      "selected_references = {\n",
      "    # 'Info-Object': 'your_chosen_filename.jpg',\n",
      "    # 'Excel-Tabelle': 'your_chosen_filename.jpg',\n",
      "    # 'Data Source': 'your_chosen_filename.jpg',\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Manual selection commands\n",
    "if category_analysis:\n",
    "    print(\"ğŸ¯ MANUAL REFERENCE SELECTION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    categories = list(category_analysis.keys())\n",
    "    print(\"ğŸ“‹ Available categories:\")\n",
    "    for i, cat in enumerate(categories, 1):\n",
    "        count = category_analysis[cat]['count']\n",
    "        print(f\"   {i}. {cat} ({count} images)\")\n",
    "    \n",
    "    print(\"\\nğŸ” TO VIEW CANDIDATES FOR A CATEGORY:\")\n",
    "    if categories:\n",
    "        example_cat = categories[0]\n",
    "        print(f\"# View top 3 candidates for {example_cat}\")\n",
    "        print(f\"if '{example_cat}' in category_analysis:\")\n",
    "        print(f\"    stats = category_analysis['{example_cat}']\")\n",
    "        print(f\"    candidates = stats['items'][:3]\")\n",
    "        print(f\"    for i, candidate in enumerate(candidates, 1):\")\n",
    "        print(f\"        print(f'{{i}}. {{candidate[\\\"filename\\\"]}} - CNN: {{candidate[\\\"cnn_confidence\\\"]:.2%}} - Claude: {{candidate[\\\"claude_score\\\"]:.1f}}/10')\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ TO SELECT A REFERENCE:\")\n",
    "    print(\"# Add your manual selections here:\")\n",
    "    print(\"selected_references = {\")\n",
    "    for cat in categories:\n",
    "        print(f\"    # '{cat}': 'your_chosen_filename.jpg',\")\n",
    "    print(\"}\")\n",
    "else:\n",
    "    print(\"âŒ No categories available for selection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” QUICK VIEW: Info-Object\n",
      "========================================\n",
      "\n",
      "1. 005a2c943c5a4344bc058a3680a62c43.png\n",
      "   ğŸ“Š CNN Confidence: 96.58%\n",
      "   ğŸ¤– Claude Score: 8.0/10\n",
      "   â­ Combined Score: 0.850\n",
      "\n",
      "2. 0056d6ebd8c045ea8bac5910b8aee061.jpeg\n",
      "   ğŸ“Š CNN Confidence: 89.97%\n",
      "   ğŸ¤– Claude Score: 6.0/10\n",
      "   â­ Combined Score: 0.690\n",
      "\n",
      "3. 001e53127c8049dc94ead93b884b92fa.jpeg\n",
      "   ğŸ“Š CNN Confidence: 91.61%\n",
      "   ğŸ¤– Claude Score: âŒ Failed\n",
      "   â­ Combined Score: 0.458\n",
      "\n",
      "ğŸ’¡ To select the best one, you can manually choose:\n",
      "# Example: selected_references = {'Info-Object': '005a2c943c5a4344bc058a3680a62c43.png'}\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Quick test - show first category candidates\n",
    "if category_analysis:\n",
    "    first_category = list(category_analysis.keys())[0]\n",
    "    print(f\"ğŸ” QUICK VIEW: {first_category}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    stats = category_analysis[first_category]\n",
    "    candidates = stats['items'][:3]\n",
    "    \n",
    "    for i, candidate in enumerate(candidates, 1):\n",
    "        print(f\"\\n{i}. {candidate['filename']}\")\n",
    "        print(f\"   ğŸ“Š CNN Confidence: {candidate['cnn_confidence']:.2%}\")\n",
    "        if candidate['claude_success']:\n",
    "            print(f\"   ğŸ¤– Claude Score: {candidate['claude_score']:.1f}/10\")\n",
    "        else:\n",
    "            print(f\"   ğŸ¤– Claude Score: âŒ Failed\")\n",
    "        print(f\"   â­ Combined Score: {candidate['combined_score']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ To select the best one, you can manually choose:\")\n",
    "    print(f\"# Example: selected_references = {{'{first_category}': '{candidates[0]['filename']}'}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– AUTOMATIC REFERENCE SELECTION ENGINE\n",
      "==================================================\n",
      "ğŸ“Š Processing 10 successful results\n",
      "\n",
      "ğŸ¯ AUTOMATIC SELECTION RESULTS:\n",
      "--------------------------------------------------\n",
      "âœ… Info-Object:\n",
      "   ğŸ“„ File: 005a2c943c5a4344bc058a3680a62c43.png\n",
      "   ğŸ“Š CNN: 96.58%\n",
      "   ğŸ¤– Claude: 8.0/10\n",
      "   â­ Score: 0.850\n",
      "\n",
      "âœ… Excel-Tabelle:\n",
      "   ğŸ“„ File: 0042c7e49bb143ca9b50b3d6336c003b.png\n",
      "   ğŸ“Š CNN: 99.86%\n",
      "   ğŸ¤– Claude: 8.0/10\n",
      "   â­ Score: 0.860\n",
      "\n",
      "âœ… Data Source:\n",
      "   ğŸ“„ File: 008e8269e68341848a3680ebc4f11910.jpeg\n",
      "   ğŸ“Š CNN: 99.82%\n",
      "   ğŸ¤– Claude: âŒ Failed\n",
      "   â­ Score: 0.499\n",
      "\n",
      "ğŸ‰ AUTOMATIC SELECTION COMPLETE!\n",
      "ğŸ“ Selected references for 3 categories\n"
     ]
    }
   ],
   "source": [
    "def automatic_reference_selection(validation_results, min_score_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Tamamen otomatik reference selection\n",
    "    En yÃ¼ksek combined score'a gÃ¶re her kategoriden en iyi adaylarÄ± seÃ§er\n",
    "    \"\"\"\n",
    "    \n",
    "    if not validation_results:\n",
    "        print(\"âŒ No validation results!\")\n",
    "        return {}\n",
    "    \n",
    "    print(\"ğŸ¤– AUTOMATIC REFERENCE SELECTION ENGINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Successful results only\n",
    "    successful_results = [r for r in validation_results if r.get('status') == 'success']\n",
    "    print(f\"ğŸ“Š Processing {len(successful_results)} successful results\")\n",
    "    \n",
    "    # Group by category\n",
    "    categories = {}\n",
    "    \n",
    "    for result in successful_results:\n",
    "        category = result['cnn_prediction']\n",
    "        if category not in categories:\n",
    "            categories[category] = []\n",
    "        \n",
    "        # Calculate scores\n",
    "        cnn_conf = result['cnn_confidence']\n",
    "        claude_analysis = result.get('claude_analysis', {})\n",
    "        \n",
    "        # Claude score\n",
    "        claude_score = 0\n",
    "        claude_success = False\n",
    "        if 'error' not in claude_analysis and 'gesamt_score' in claude_analysis:\n",
    "            claude_score = claude_analysis['gesamt_score']\n",
    "            claude_success = True\n",
    "        \n",
    "        # Combined score calculation\n",
    "        if claude_success:\n",
    "            combined_score = (cnn_conf * 0.3) + (claude_score/10 * 0.7)  # Claude weighted more\n",
    "        else:\n",
    "            combined_score = cnn_conf * 0.5  # Penalty for Claude failure\n",
    "        \n",
    "        candidate = {\n",
    "            'filename': result['filename'],\n",
    "            'cnn_confidence': cnn_conf,\n",
    "            'claude_score': claude_score,\n",
    "            'claude_success': claude_success,\n",
    "            'combined_score': combined_score,\n",
    "            'full_result': result\n",
    "        }\n",
    "        \n",
    "        categories[category].append(candidate)\n",
    "    \n",
    "    # Select best reference for each category\n",
    "    selected_references = {}\n",
    "    \n",
    "    print(f\"\\nğŸ¯ AUTOMATIC SELECTION RESULTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for category, candidates in categories.items():\n",
    "        # Sort by combined score (highest first)\n",
    "        sorted_candidates = sorted(candidates, key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        # Get best candidate\n",
    "        best_candidate = sorted_candidates[0]\n",
    "        \n",
    "        # Quality check\n",
    "        if best_candidate['combined_score'] >= min_score_threshold:\n",
    "            selected_references[category] = {\n",
    "                'filename': best_candidate['filename'],\n",
    "                'selection_reason': f\"Highest combined score: {best_candidate['combined_score']:.3f}\",\n",
    "                'cnn_confidence': best_candidate['cnn_confidence'],\n",
    "                'claude_score': best_candidate['claude_score'],\n",
    "                'claude_success': best_candidate['claude_success'],\n",
    "                'combined_score': best_candidate['combined_score'],\n",
    "                'selected_at': datetime.now().isoformat(),\n",
    "                'selection_method': 'automatic_best_score'\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… {category}:\")\n",
    "            print(f\"   ğŸ“„ File: {best_candidate['filename']}\")\n",
    "            print(f\"   ğŸ“Š CNN: {best_candidate['cnn_confidence']:.2%}\")\n",
    "            if best_candidate['claude_success']:\n",
    "                print(f\"   ğŸ¤– Claude: {best_candidate['claude_score']:.1f}/10\")\n",
    "            else:\n",
    "                print(f\"   ğŸ¤– Claude: âŒ Failed\")\n",
    "            print(f\"   â­ Score: {best_candidate['combined_score']:.3f}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"âš ï¸ {category}: Best score {best_candidate['combined_score']:.3f} below threshold {min_score_threshold}\")\n",
    "    \n",
    "    return selected_references\n",
    "\n",
    "# Execute automatic selection\n",
    "automatic_references = automatic_reference_selection(validation_results)\n",
    "\n",
    "print(f\"ğŸ‰ AUTOMATIC SELECTION COMPLETE!\")\n",
    "print(f\"ğŸ“ Selected references for {len(automatic_references)} categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AUTOMATIC REFERENCES SAVED!\n",
      "ğŸ“„ File: reference_solutions_automatic_20250619_193609.json\n",
      "ğŸ“Š Categories: 3\n",
      "ğŸ“‹ Summary: automatic_selection_summary_20250619_193609.txt\n",
      "\n",
      "ğŸ“ˆ SELECTION STATISTICS:\n",
      "   ğŸ¯ Categories processed: 3\n",
      "   ğŸ“Š Total candidates considered: 10\n",
      "   â­ Average combined score: 0.736\n",
      "   ğŸ¤– Claude success rate: 66.7%\n",
      "\n",
      "ğŸš€ NEXT STEPS:\n",
      "   1ï¸âƒ£ References are ready for comparison_engine.ipynb\n",
      "   2ï¸âƒ£ Run aufgabe4_final_evaluation.ipynb for testing\n",
      "   3ï¸âƒ£ Reference file: reference_solutions_automatic_20250619_193609.json\n"
     ]
    }
   ],
   "source": [
    "# Save automatic reference selections\n",
    "if automatic_references:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"reference_solutions_automatic_{timestamp}.json\"\n",
    "    output_path = os.path.join(REFERENCE_OUTPUT, output_filename)\n",
    "    \n",
    "    # Prepare final reference data\n",
    "    reference_data = {\n",
    "        'created_at': timestamp,\n",
    "        'selection_method': 'automatic_best_score',\n",
    "        'total_categories': len(automatic_references),\n",
    "        'total_references': len(automatic_references),\n",
    "        'selection_criteria': {\n",
    "            'cnn_weight': 0.3,\n",
    "            'claude_weight': 0.7,\n",
    "            'min_threshold': 0.4,\n",
    "            'penalty_for_claude_failure': 0.5\n",
    "        },\n",
    "        'references': {\n",
    "            category: [selection] for category, selection in automatic_references.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(reference_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… AUTOMATIC REFERENCES SAVED!\")\n",
    "    print(f\"ğŸ“„ File: {output_filename}\")\n",
    "    print(f\"ğŸ“Š Categories: {len(automatic_references)}\")\n",
    "    \n",
    "    # Create summary file\n",
    "    summary_filename = f\"automatic_selection_summary_{timestamp}.txt\"\n",
    "    summary_path = os.path.join(REFERENCE_OUTPUT, summary_filename)\n",
    "    \n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"AUTOMATIC REFERENCE SELECTION SUMMARY\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(f\"Selection Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Selection Method: Automatic (Best Combined Score)\\n\")\n",
    "        f.write(f\"Total Categories: {len(automatic_references)}\\n\\n\")\n",
    "        \n",
    "        for category, selection in automatic_references.items():\n",
    "            f.write(f\"ğŸ“ {category}:\\n\")\n",
    "            f.write(f\"   âœ… {selection['filename']}\\n\")\n",
    "            f.write(f\"   ğŸ“Š CNN: {selection['cnn_confidence']:.2%}\\n\")\n",
    "            if selection['claude_success']:\n",
    "                f.write(f\"   ğŸ¤– Claude: {selection['claude_score']:.1f}/10\\n\")\n",
    "            else:\n",
    "                f.write(f\"   ğŸ¤– Claude: âŒ Failed\\n\")\n",
    "            f.write(f\"   â­ Combined Score: {selection['combined_score']:.3f}\\n\")\n",
    "            f.write(f\"   ğŸ“ Reason: {selection['selection_reason']}\\n\\n\")\n",
    "    \n",
    "    print(f\"ğŸ“‹ Summary: {summary_filename}\")\n",
    "    \n",
    "    # Show final statistics\n",
    "    total_candidates = sum(len(category_analysis[cat]['items']) for cat in automatic_references.keys())\n",
    "    avg_score = sum(ref['combined_score'] for ref in automatic_references.values()) / len(automatic_references)\n",
    "    claude_success_rate = sum(1 for ref in automatic_references.values() if ref['claude_success']) / len(automatic_references) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ SELECTION STATISTICS:\")\n",
    "    print(f\"   ğŸ¯ Categories processed: {len(automatic_references)}\")\n",
    "    print(f\"   ğŸ“Š Total candidates considered: {total_candidates}\")\n",
    "    print(f\"   â­ Average combined score: {avg_score:.3f}\")\n",
    "    print(f\"   ğŸ¤– Claude success rate: {claude_success_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nğŸš€ NEXT STEPS:\")\n",
    "    print(f\"   1ï¸âƒ£ References are ready for comparison_engine.ipynb\")\n",
    "    print(f\"   2ï¸âƒ£ Run aufgabe4_final_evaluation.ipynb for testing\")\n",
    "    print(f\"   3ï¸âƒ£ Reference file: {output_filename}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No references were automatically selected!\")\n",
    "    print(\"ğŸ’¡ Check validation results and try lowering the threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” QUALITY VERIFICATION OF AUTOMATIC SELECTIONS\n",
      "==================================================\n",
      "\n",
      "ğŸ“ Info-Object:\n",
      "   ğŸ“„ Selected: 005a2c943c5a4344bc058a3680a62c43.png\n",
      "   âœ… File exists (34.3 KB)\n",
      "   ğŸ“Š Quality scores:\n",
      "      CNN Confidence: 96.58%\n",
      "      Claude Score: 8.0/10\n",
      "      Combined Score: 0.850\n",
      "\n",
      "ğŸ“ Excel-Tabelle:\n",
      "   ğŸ“„ Selected: 0042c7e49bb143ca9b50b3d6336c003b.png\n",
      "   âœ… File exists (254.1 KB)\n",
      "   ğŸ“Š Quality scores:\n",
      "      CNN Confidence: 99.86%\n",
      "      Claude Score: 8.0/10\n",
      "      Combined Score: 0.860\n",
      "\n",
      "ğŸ“ Data Source:\n",
      "   ğŸ“„ Selected: 008e8269e68341848a3680ebc4f11910.jpeg\n",
      "   âœ… File exists (88.9 KB)\n",
      "   ğŸ“Š Quality scores:\n",
      "      CNN Confidence: 99.82%\n",
      "      Claude Score: âŒ Failed (using CNN only)\n",
      "      Combined Score: 0.499 (penalized)\n",
      "\n",
      "ğŸ‰ AUTOMATIC REFERENCE SELECTION COMPLETED!\n",
      "âœ… Ready for Aufgabe 4 comparison engine!\n"
     ]
    }
   ],
   "source": [
    "# Verify automatic selections\n",
    "if automatic_references:\n",
    "    print(\"ğŸ” QUALITY VERIFICATION OF AUTOMATIC SELECTIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, selection in automatic_references.items():\n",
    "        print(f\"\\nğŸ“ {category}:\")\n",
    "        print(f\"   ğŸ“„ Selected: {selection['filename']}\")\n",
    "        \n",
    "        # Check if file exists\n",
    "        file_path = os.path.join(VAL_DIR, selection['filename'])\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path) / 1024\n",
    "            print(f\"   âœ… File exists ({file_size:.1f} KB)\")\n",
    "        else:\n",
    "            print(f\"   âŒ File not found!\")\n",
    "        \n",
    "        # Quality scores\n",
    "        print(f\"   ğŸ“Š Quality scores:\")\n",
    "        print(f\"      CNN Confidence: {selection['cnn_confidence']:.2%}\")\n",
    "        if selection['claude_success']:\n",
    "            print(f\"      Claude Score: {selection['claude_score']:.1f}/10\")\n",
    "            print(f\"      Combined Score: {selection['combined_score']:.3f}\")\n",
    "        else:\n",
    "            print(f\"      Claude Score: âŒ Failed (using CNN only)\")\n",
    "            print(f\"      Combined Score: {selection['combined_score']:.3f} (penalized)\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ AUTOMATIC REFERENCE SELECTION COMPLETED!\")\n",
    "    print(f\"âœ… Ready for Aufgabe 4 comparison engine!\")\n",
    "else:\n",
    "    print(\"âŒ No automatic selections available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
