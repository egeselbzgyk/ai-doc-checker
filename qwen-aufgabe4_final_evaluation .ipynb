{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 🎓 Aufgabe 4 - Final Evaluation System\n",
    "\n",
    "**Vergleich mit Musterlösung - Complete System Test**\n",
    "\n",
    "Bu notebook **Aufgabe 4**'ün final implementation'ı:\n",
    "\n",
    "1. **Complete Workflow Test** - Ende-zu-Ende sistem testi\n",
    "2. **Student vs Reference** karşılaştırması  \n",
    "3. **Automated Grading** sistemi\n",
    "4. **Results Export** - PDF raporlar\n",
    "5. **Performance Analysis** - sistem değerlendirmesi\n",
    "\n",
    "## 🏆 Das ist der finale Schritt für Aufgabe 4!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Aufgabe 4 Final Evaluation System\n",
      "🎯 Ready to test complete workflow!\n",
      "📁 Validation images: C:\\Users\\egese\\Desktop\\dataset\\val\\SAP\n",
      "📊 Analysis results: results/validation_analysis/\n",
      "🎯 Reference solutions: results/reference_solutions/\n",
      "🏆 Final output: results/aufgabe4_final/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Local imports\n",
    "from predict import predict_image\n",
    "from prompt_templates import prompt_templates, comparison_prompt\n",
    "\n",
    "print(\"✅ Aufgabe 4 Final Evaluation System\")\n",
    "print(\"🎯 Ready to test complete workflow!\")\n",
    "\n",
    "# Setup paths\n",
    "VAL_DIR = r\"C:\\Users\\egese\\Desktop\\dataset\\val\\SAP\"          # Student solutions (only for the test)\n",
    "RESULTS_DIR = \"results/validation_analysis/\"\n",
    "REFERENCE_DIR = \"results/reference_solutions/\"\n",
    "FINAL_OUTPUT = \"results/aufgabe4_final/\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(FINAL_OUTPUT, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Validation images: {VAL_DIR}\")\n",
    "print(f\"📊 Analysis results: {RESULTS_DIR}\")\n",
    "print(f\"🎯 Reference solutions: {REFERENCE_DIR}\")\n",
    "print(f\"🏆 Final output: {FINAL_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🎯 Complete Evaluation Class\n",
    "\n",
    "Dies ist die finale Implementation von Aufgabe 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API ready for evaluation (Model: qwen/qwen2.5-vl-32b-instruct)\n",
      "🔧 Using Qwen2.5-VL-32B for improved JSON processing\n",
      "✅ Found reference in directory: reference_database_latest.json\n",
      "✅ Loaded reference database: 6 categories\n",
      "   📁 Data Source: 5 reference(s)\n",
      "   📁 Data-Flow: 5 reference(s)\n",
      "   📁 Data-Transfer-Process: 5 reference(s)\n",
      "   📁 Excel-Tabelle: 5 reference(s)\n",
      "   📁 Info-Object: 5 reference(s)\n",
      "   📁 Transformation: 5 reference(s)\n",
      "\n",
      "🎓 Aufgabe 4 Evaluation System ready!\n",
      "📊 Categories with references: 6\n"
     ]
    }
   ],
   "source": [
    "class Aufgabe4EvaluationSystem:\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"Complete evaluation system for Aufgabe 4\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.setup_api()\n",
    "        \n",
    "        # Load reference solutions\n",
    "        self.reference_solutions = {}\n",
    "        self.load_reference_database()\n",
    "        \n",
    "        # Evaluation results\n",
    "        self.evaluation_results = []\n",
    "        \n",
    "    def setup_api(self):\n",
    "        \"\"\"Setup OpenRouter API\"\"\"\n",
    "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        # Use more reliable model for complex multi-image tasks\n",
    "        self.model = \"qwen/qwen2.5-vl-32b-instruct\" # Better multi-image support\n",
    "        # Alternative models if current fails:\n",
    "        # self.model = \"qwen/qwen-vl-max\" # Backup model 1\n",
    "        # self.model = \"anthropic/claude-3-sonnet:beta\" # Backup model 2\n",
    "        print(f\"✅ API ready for evaluation (Model: {self.model})\")\n",
    "        print(\"🔧 Using Qwen2.5-VL-32B for improved JSON processing\")\n",
    "    \n",
    "    def switch_model(self, model_name):\n",
    "        \"\"\"Switch to a different model if current one has issues\"\"\"\n",
    "        old_model = self.model\n",
    "        self.model = model_name\n",
    "        print(f\"🔄 Switched model: {old_model} → {model_name}\")\n",
    "    \n",
    "    def load_reference_database(self):\n",
    "        \"\"\"Load reference solutions from JSON files\"\"\"\n",
    "        # Try multiple possible locations for reference database\n",
    "        possible_paths = [\n",
    "            \"reference_database_latest.json\",\n",
    "            \"reference_database_auto_*.json\", \n",
    "            os.path.join(REFERENCE_DIR, \"reference_solutions_*.json\"),\n",
    "            os.path.join(REFERENCE_DIR, \"reference_database_*.json\")\n",
    "        ]\n",
    "        \n",
    "        reference_file = None\n",
    "        \n",
    "        # Check for latest reference database in current directory first\n",
    "        if os.path.exists(\"reference_database_latest.json\"):\n",
    "            reference_file = \"reference_database_latest.json\"\n",
    "            print(\"✅ Found reference_database_latest.json\")\n",
    "        else:\n",
    "            # Look for auto-generated files\n",
    "            import glob\n",
    "            auto_files = glob.glob(\"reference_database_auto_*.json\")\n",
    "            if auto_files:\n",
    "                reference_file = sorted(auto_files)[-1]  # Get latest\n",
    "                print(f\"✅ Found auto-generated reference: {reference_file}\")\n",
    "            else:\n",
    "                # Check reference directory if it exists\n",
    "                if os.path.exists(REFERENCE_DIR):\n",
    "                    ref_files = [f for f in os.listdir(REFERENCE_DIR) if f.startswith('reference_')]\n",
    "                    if ref_files:\n",
    "                        latest_ref = sorted(ref_files)[-1]\n",
    "                        reference_file = os.path.join(REFERENCE_DIR, latest_ref)\n",
    "                        print(f\"✅ Found reference in directory: {latest_ref}\")\n",
    "        \n",
    "        if reference_file and os.path.exists(reference_file):\n",
    "            try:\n",
    "                with open(reference_file, 'r', encoding='utf-8') as f:\n",
    "                    ref_data = json.load(f)\n",
    "                    \n",
    "                # Handle different JSON structures\n",
    "                if 'references' in ref_data:\n",
    "                    self.reference_solutions = ref_data['references']\n",
    "                elif 'categories' in ref_data:\n",
    "                    self.reference_solutions = ref_data['categories']\n",
    "                else:\n",
    "                    # Assume the data itself is the reference structure\n",
    "                    self.reference_solutions = ref_data\n",
    "                \n",
    "                print(f\"✅ Loaded reference database: {len(self.reference_solutions)} categories\")\n",
    "                for category, refs in self.reference_solutions.items():\n",
    "                    if isinstance(refs, list):\n",
    "                        print(f\"   📁 {category}: {len(refs)} reference(s)\")\n",
    "                    else:\n",
    "                        print(f\"   📁 {category}: {type(refs)} data\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading reference database: {e}\")\n",
    "                self.reference_solutions = {}\n",
    "        else:\n",
    "            print(\"❌ No reference database found!\")\n",
    "            print(\"💡 Available files:\", [f for f in os.listdir('.') if 'reference' in f])\n",
    "            self.reference_solutions = {}\n",
    "    \n",
    "    def classify_student_submission(self, image_path):\n",
    "        \"\"\"CNN classification of student submission\"\"\"\n",
    "        try:\n",
    "            predicted_class, confidence = predict_image(image_path)\n",
    "            return {\n",
    "                \"category\": predicted_class,\n",
    "                \"confidence\": float(confidence),\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"category\": None,\n",
    "                \"confidence\": 0.0,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def get_best_reference(self, category):\n",
    "        \"\"\"Get best reference for a category\"\"\"\n",
    "        if category not in self.reference_solutions:\n",
    "            return None\n",
    "        \n",
    "        references = self.reference_solutions[category]\n",
    "        if not references:\n",
    "            return None\n",
    "        \n",
    "        # Get first (best) reference\n",
    "        best_ref = references[0]\n",
    "        ref_filename = best_ref['filename']\n",
    "        \n",
    "        # Reference images are in mapped_train, organized by category\n",
    "        # Try different possible paths\n",
    "        possible_paths = [\n",
    "            os.path.join(r\"C:\\Users\\egese\\Desktop\\dataset\\mapped_train\", category, ref_filename),\n",
    "            os.path.join(r\"C:\\Users\\egese\\Desktop\\dataset\\mapped_train\", category.replace(\" \", \"-\"), ref_filename),\n",
    "            os.path.join(r\"C:\\Users\\egese\\Desktop\\dataset\\mapped_train\", category.replace(\"-\", \" \"), ref_filename),\n",
    "            os.path.join(VAL_DIR, ref_filename),  # Fallback to val directory\n",
    "        ]\n",
    "        \n",
    "        for ref_path in possible_paths:\n",
    "            if os.path.exists(ref_path):\n",
    "                return {\n",
    "                    \"filename\": ref_filename,\n",
    "                    \"path\": ref_path,\n",
    "                    \"details\": best_ref,\n",
    "                    \"category_path\": category\n",
    "                }\n",
    "        \n",
    "        print(f\"⚠️ Reference image not found: {ref_filename} for category {category}\")\n",
    "        print(f\"   Tried paths: {possible_paths}\")\n",
    "        return None\n",
    "    \n",
    "    def get_image_media_type(self, image_path):\n",
    "        \"\"\"Detect actual image format and return correct media type\"\"\"\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                format_name = img.format.lower()\n",
    "                if format_name == 'jpeg':\n",
    "                    return 'image/jpeg'\n",
    "                elif format_name == 'png':\n",
    "                    return 'image/png'\n",
    "                elif format_name == 'gif':\n",
    "                    return 'image/gif'\n",
    "                elif format_name == 'webp':\n",
    "                    return 'image/webp'\n",
    "                else:\n",
    "                    return 'image/jpeg'  # Default fallback\n",
    "        except Exception:\n",
    "            # Fallback based on file extension\n",
    "            ext = os.path.splitext(image_path)[1].lower()\n",
    "            if ext in ['.png']:\n",
    "                return 'image/png'\n",
    "            elif ext in ['.gif']:\n",
    "                return 'image/gif'\n",
    "            elif ext in ['.webp']:\n",
    "                return 'image/webp'\n",
    "            else:\n",
    "                return 'image/jpeg'  # Default\n",
    "    \n",
    "    def encode_image(self, image_path):\n",
    "        \"\"\"Encode image to base64 with correct media type\"\"\"\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                base64_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "                media_type = self.get_image_media_type(image_path)\n",
    "                return base64_data, media_type\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Image encoding error: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def get_category_prompt(self, category):\n",
    "        \"\"\"Get category-specific prompt from prompt_templates.py\"\"\"\n",
    "        # Mapping zwischen CNN predictions und prompt_templates keys\n",
    "        category_mapping = {\n",
    "            \"Excel-Tabelle\": \"Excel-Tabelle\",\n",
    "            \"Data-Flow\": \"Data-Flow\", \n",
    "            \"Data-Transfer-Process\": \"Data-Transfer-Process\",\n",
    "            \"Transformation\": \"Transformation\",\n",
    "            \"Data Source\": \"Data Source\",\n",
    "            \"Info-Object\": \"Info-Object\",\n",
    "            # Alternative Namensgebungen falls nötig\n",
    "            \"Data-Transfer\": \"Data-Transfer-Process\",\n",
    "            \"DataFlow\": \"Data-Flow\",\n",
    "            \"DataSource\": \"Data Source\",\n",
    "            \"InfoObject\": \"Info-Object\",\n",
    "            \"Excel\": \"Excel-Tabelle\"\n",
    "        }\n",
    "        \n",
    "        # Richtige Kategorie finden\n",
    "        mapped_category = category_mapping.get(category, category)\n",
    "        \n",
    "        if mapped_category in prompt_templates:\n",
    "            print(f\"   📋 Using category-specific prompt for: {mapped_category}\")\n",
    "            return prompt_templates[mapped_category]\n",
    "        else:\n",
    "            print(f\"⚠️ No specific prompt for category '{category}', using generic prompt\")\n",
    "            return self.get_generic_prompt()\n",
    "    \n",
    "    def get_generic_prompt(self):\n",
    "        \"\"\"Generic prompt if category-specific not found\"\"\"\n",
    "        return \"\"\"\n",
    "Analysiere das Bild und bewerte nach folgenden Kriterien:\n",
    "{\n",
    "  \"struktur_qualitaet\": {\n",
    "    \"aufbau_logisch\": true/false,\n",
    "    \"elemente_erkennbar\": true/false,\n",
    "    \"vollstaendigkeit\": 0-10,\n",
    "    \"score\": 0-10\n",
    "  },\n",
    "  \"technische_qualitaet\": {\n",
    "    \"lesbarkeit\": \"gut/mittel/schlecht\",\n",
    "    \"detailgrad\": \"zu wenig/angemessen/zu viel\", \n",
    "    \"fachliche_korrektheit\": 0-10,\n",
    "    \"score\": 0-10\n",
    "  },\n",
    "  \"sap_kontext\": {\n",
    "    \"sap_bw_relevant\": true/false,\n",
    "    \"terminologie_korrekt\": true/false,\n",
    "    \"business_kontext\": \"erkennbar/unklar\",\n",
    "    \"score\": 0-10\n",
    "  },\n",
    "  \"gesamt_score\": 0-10,\n",
    "  \"verbesserungsvorschlaege\": [\"Konkrete Hinweise\"]\n",
    "}\n",
    "\"\"\",\n",
    "    \n",
    "    def _make_api_call(self, messages, max_tokens=2048):\n",
    "        \"\"\"Helper function to make a generic API call.\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"http://localhost:8888\", # Or your app's URL\n",
    "            \"X-Title\": \"Aufgabe 4 Evaluation\"\n",
    "        }\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.1,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.base_url, headers=headers, json=data, timeout=120)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "                return {\"response\": content}\n",
    "            else:\n",
    "                return {\"error\": f\"API Error HTTP {response.status_code}: {response.text}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Request Exception: {str(e)}\"}\n",
    "\n",
    "    def _check_image_quality(self, image_path):\n",
    "        \"\"\"Check if image is suitable for evaluation (Schritt 1)\"\"\"\n",
    "        print(f\"   Prüfe Bildqualität: {os.path.basename(image_path)}\")\n",
    "        base64_image, media_type = self.encode_image(image_path)\n",
    "        if not base64_image:\n",
    "            return {\"error\": \"Failed to encode image\"}\n",
    "\n",
    "        prompt_text = \"\"\"SCHRITT 1: PRÜFUNG DER EIGNUNG ZUR BEWERTUNG\n",
    "\n",
    "Prüfe, ob das Studenten-Bild für eine Bewertung GEEIGNET ist. \n",
    "Ein Bild ist **NICHT GEEIGNET** (`\"is_evaluable\": false`), wenn einer der folgenden Punkte zutrifft:\n",
    "\n",
    "**Fokus auf UI-Elemente statt Inhalt:**\n",
    "- Das Bild zeigt hauptsächlich ein Menü (Rechtsklick), Dropdown-Liste, Dialogbox oder Popup-Fehlermeldung\n",
    "- Das Bild zeigt den Login-Screen, SAP-Startseite (Easy Access) oder generische Transaktionsauswahl\n",
    "\n",
    "**Qualität und Lesbarkeit:**\n",
    "- Das Bild ist stark verpixelt, unscharf oder niedrig aufgelöst\n",
    "- Das Bild ist extrem dunkel, überbelichtet oder hat geringen Kontrast\n",
    "\n",
    "**Falscher Bildausschnitt:**\n",
    "- Das Bild ist zu stark herangezoomt (nur winziges Detail sichtbar)\n",
    "- Das Bild ist zu weit herausgezoomt (SAP-Fenster sehr klein im Screenshot)\n",
    "\n",
    "**Irrelevanter Inhalt:**\n",
    "- Das Bild zeigt ABAP-Code statt grafisches Modell\n",
    "- Das Bild zeigt andere Anwendung (Windows Explorer, etc.)\n",
    "- Das Bild zeigt leeren/Lade-Bildschirm oder ist unvollständig\n",
    "- Es gibt irrelevante Inhalte auf dem Bildschirm\n",
    "\n",
    "Gib NUR folgendes JSON zurück:\n",
    "\n",
    "{\n",
    "  \"is_evaluable\": true/false,\n",
    "  \"reason\": \"Kurze Begründung warum geeignet/nicht geeignet\"\n",
    "}\n",
    "\n",
    "WICHTIG: Antworte NUR mit JSON - keine Markdown-Blöcke, keine Erklärung.\"\"\"\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{media_type};base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": prompt_text}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        quality_result = self._make_api_call(messages)\n",
    "        return self.parse_evaluation_response(quality_result)\n",
    "\n",
    "    def _analyze_single_image(self, image_path, category_prompt):\n",
    "        \"\"\"Analyzes a single image using the LLM and returns the JSON analysis.\"\"\"\n",
    "        print(f\"   Analysiere Bild: {os.path.basename(image_path)}\")\n",
    "        base64_image, media_type = self.encode_image(image_path)\n",
    "        if not base64_image:\n",
    "            return {\"error\": \"Failed to encode image\"}\n",
    "\n",
    "        prompt_text = f\"\"\"Du bist ein SAP BW Experte. Analysiere das Bild detailliert und gib deine Bewertung als JSON zurück.\n",
    "\n",
    "BEWERTUNGSSCHEMA (exakt in diesem Format antworten):\n",
    "{category_prompt}\n",
    "\n",
    "WICHTIGE REGELN:\n",
    "- Verwende EXAKT die Felder aus dem Schema oben\n",
    "- Alle score-Werte sind zwischen 0-10\n",
    "- Alle true/false Werte sind boolean\n",
    "- Alle Textwerte sind Strings in Anführungszeichen\n",
    "- Gib NUR das JSON zurück - keine Markdown-Blöcke, keine Erklärung\n",
    "\n",
    "Antworte NUR mit dem JSON-Objekt:\"\"\"\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{media_type};base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": prompt_text}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        analysis_result = self._make_api_call(messages)\n",
    "        return self.parse_evaluation_response(analysis_result)\n",
    "\n",
    "    def debug_single_image_passthrough(self, image_path):\n",
    "        \"\"\"A simple test to see if the model can see ONE image.\"\"\"\n",
    "        print(\"\\n--- 🕵️ RUNNING SINGLE IMAGE DEBUG TEST ---\")\n",
    "        student_b64, student_media_type = self.encode_image(image_path)\n",
    "        if not student_b64:\n",
    "            return {\"error\": \"Failed to encode image\"}\n",
    "\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{student_media_type};base64,{student_b64}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail in English. What do you see?\"}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        try:\n",
    "            print(\"   ...Sending request to model...\")\n",
    "            result = self._make_api_call(messages, max_tokens=1024)\n",
    "            \n",
    "            if \"error\" not in result:\n",
    "                print(\"\\n--- ✅ Model Response ---\")\n",
    "                print(result[\"response\"])\n",
    "                return result\n",
    "            else:\n",
    "                print(\"\\n--- ❌ Error Response ---\")\n",
    "                print(result[\"error\"])\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Request failed: {str(e)}\"}\n",
    "\n",
    "\n",
    "    \n",
    "    def compare_with_reference(self, student_path, reference_info, category):\n",
    "        \"\"\"Compare student submission with reference using 4-step approach with quality check\"\"\"\n",
    "        \n",
    "        # STEP 0: Check if student image is evaluable (quality control)\n",
    "        quality_check = self._check_image_quality(student_path)\n",
    "        if \"error\" in quality_check:\n",
    "            return quality_check\n",
    "            \n",
    "        # If image is not suitable for evaluation, return early with detailed reason\n",
    "        if not quality_check.get(\"is_evaluable\", False):\n",
    "            return {\n",
    "                \"qualitaetspruefung\": {\n",
    "                    \"is_evaluable\": False,\n",
    "                    \"reason\": quality_check.get('reason', 'Bild nicht für Bewertung geeignet')\n",
    "                },\n",
    "                \"score\": 0,\n",
    "                \"detailed_scores\": {\n",
    "                    \"structure\": 0,\n",
    "                    \"technical_quality\": 0, \n",
    "                    \"completeness\": 0,\n",
    "                    \"correctness\": 0\n",
    "                },\n",
    "                \"strengths\": [],\n",
    "                \"weaknesses\": [\"Bild ist nicht für Bewertung geeignet\"],\n",
    "                \"suggestions\": [\"Bitte ein aussagekräftiges Screenshot der SAP BW Modellierung einreichen\"],\n",
    "                \"category_specific_feedback\": f\"Nicht bewertbar: {quality_check.get('reason', 'Unbekannter Grund')}\",\n",
    "                \"evaluation_status\": \"not_evaluable\"\n",
    "            }\n",
    "        \n",
    "        print(f\"   ✅ Bildqualität OK: {quality_check.get('reason', '')}\")\n",
    "        \n",
    "        # STEP 1: Analyze student image\n",
    "        category_prompt = self.get_category_prompt(category)\n",
    "        student_analysis = self._analyze_single_image(student_path, category_prompt)\n",
    "        if \"error\" in student_analysis:\n",
    "            return {\"error\": f\"Student image analysis failed: {student_analysis['error']}\"}\n",
    "\n",
    "        # STEP 2: Analyze reference image  \n",
    "        reference_analysis = self._analyze_single_image(reference_info['path'], category_prompt)\n",
    "        if \"error\" in reference_analysis:\n",
    "            return {\"error\": f\"Reference image analysis failed: {reference_analysis['error']}\"}\n",
    "        \n",
    "        # STEP 3: Compare the two analyses using SAME category-specific format\n",
    "        print(\"   Vergleiche die beiden Analysen...\")\n",
    "        \n",
    "        # Get the category-specific template for comparison format\n",
    "        comparison_template = self.get_category_prompt(category)\n",
    "        \n",
    "        comparison_prompt_template = \"\"\"Du bist ein SAP BW Experte. Vergleiche die beiden kategorie-spezifischen Bildanalysen der Kategorie \"{category}\".\n",
    "\n",
    "=== STUDENT ANALYSE ===\n",
    "{student_analysis}\n",
    "\n",
    "=== REFERENZ ANALYSE (Musterlösung) ===\n",
    "{reference_analysis}\n",
    "\n",
    "AUFGABE:\n",
    "1. Vergleiche die beiden Analysen Punkt für Punkt\n",
    "2. Bewerte jeden Aspekt basierend auf der Referenz  \n",
    "3. Gib das Ergebnis im GLEICHEN Format wie die ursprünglichen Analysen zurück\n",
    "\n",
    "WICHTIG: Verwende EXAKT das gleiche JSON-Schema wie die Einzelanalysen:\n",
    "{comparison_template}\n",
    "\n",
    "Aber ersetze die Werte mit BEWERTUNGEN statt Analyseergebnissen:\n",
    "- Für score-Felder: Gib Punkte 0-10 basierend auf Vergleich mit Referenz\n",
    "- Für boolean-Felder: true wenn Student gut abschneidet, false wenn schlecht\n",
    "- Für Text-Felder: Bewertungskommentare statt Analyseergebnisse\n",
    "- Für Array-Felder: Konkrete Verbesserungsvorschläge\n",
    "\n",
    "Zusätzlich füge am Ende hinzu:\n",
    "\"bewertungs_zusammenfassung\": {{\n",
    "  \"gesamtpunktzahl\": <0-100>,\n",
    "  \"note\": <1.0-5.0>,\n",
    "  \"bestanden\": true/false,\n",
    "  \"hauptkritikpunkte\": [\"Punkt 1\", \"Punkt 2\"],\n",
    "  \"verbesserungsempfehlungen\": [\"Empfehlung 1\", \"Empfehlung 2\"]\n",
    "}}\n",
    "\n",
    "Antworte NUR mit dem JSON-Objekt (keine Markdown-Blöcke):\"\"\"\n",
    "\n",
    "        comparison_prompt = comparison_prompt_template.format(\n",
    "            student_analysis=json.dumps(student_analysis, indent=2),\n",
    "            reference_analysis=json.dumps(reference_analysis, indent=2),\n",
    "            category=category,\n",
    "            comparison_template=comparison_template\n",
    "        )\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\"type\": \"text\", \"text\": comparison_prompt}]\n",
    "        }]\n",
    "        \n",
    "        comparison_result = self._make_api_call(messages)\n",
    "        parsed_result = self.parse_evaluation_response(comparison_result)\n",
    "        \n",
    "        # Add quality check info to result\n",
    "        if isinstance(parsed_result, dict) and \"error\" not in parsed_result:\n",
    "            parsed_result[\"qualitaetspruefung\"] = quality_check\n",
    "            parsed_result[\"evaluation_status\"] = \"evaluated\"\n",
    "            \n",
    "        return parsed_result\n",
    "    \n",
    "    def parse_evaluation_response(self, response):\n",
    "        \"\"\"Parse LLM evaluation response to JSON\"\"\"\n",
    "        if \"error\" in response:\n",
    "            return response\n",
    "        \n",
    "        text_output = response.get('response', '')\n",
    "        \n",
    "        # Clean up common LLM formatting issues\n",
    "        text_output = text_output.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if text_output.startswith('```json'):\n",
    "            text_output = text_output[7:]  # Remove ```json\n",
    "        if text_output.startswith('```'):\n",
    "            text_output = text_output[3:]   # Remove ```\n",
    "        if text_output.endswith('```'):\n",
    "            text_output = text_output[:-3]  # Remove trailing ```\n",
    "        \n",
    "        # Find the JSON object\n",
    "        start_idx = text_output.find('{')\n",
    "        \n",
    "        if start_idx != -1:\n",
    "            # Find the matching closing brace\n",
    "            brace_count = 0\n",
    "            end_idx = start_idx\n",
    "            for i, char in enumerate(text_output[start_idx:], start_idx):\n",
    "                if char == '{':\n",
    "                    brace_count += 1\n",
    "                elif char == '}':\n",
    "                    brace_count -= 1\n",
    "                    if brace_count == 0:\n",
    "                        end_idx = i + 1\n",
    "                        break\n",
    "            \n",
    "            if brace_count == 0:  # Found matching closing brace\n",
    "                json_str = text_output[start_idx:end_idx]\n",
    "                try:\n",
    "                    parsed = json.loads(json_str)\n",
    "                    return parsed\n",
    "                except json.JSONDecodeError as e:\n",
    "                    return {\n",
    "                        \"error\": f\"JSON parse error: {str(e)}\",\n",
    "                        \"raw_response\": json_str[:500],\n",
    "                        \"full_response\": text_output[:1000]\n",
    "                    }\n",
    "        \n",
    "        return {\n",
    "            \"error\": \"Valid JSON not found in response\",\n",
    "            \"raw_text\": text_output[:500]\n",
    "        }\n",
    "    \n",
    "    def evaluate_student_submission(self, student_image_path, student_filename):\n",
    "        \"\"\"Complete evaluation of a single student submission\"\"\"\n",
    "        print(f\"🔍 Evaluating: {student_filename}\")\n",
    "        \n",
    "        # Define a confidence threshold\n",
    "        CONFIDENCE_THRESHOLD = 0.60\n",
    "        \n",
    "        # Step 1: CNN Classification\n",
    "        classification = self.classify_student_submission(student_image_path)\n",
    "        if classification['status'] != 'success':\n",
    "            return {\n",
    "                \"filename\": student_filename,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"CNN classification failed\",\n",
    "                \"classification\": classification\n",
    "            }\n",
    "        \n",
    "        category = classification['category']\n",
    "        confidence = classification['confidence']\n",
    "        print(f\"   📊 Classified as: {category} ({confidence:.2%})\")\n",
    "        \n",
    "        # **NEW: Quality Gate based on Confidence Score**\n",
    "        if confidence < CONFIDENCE_THRESHOLD:\n",
    "            print(f\"   ⚠️ Low confidence score ({confidence:.2%}). Evaluation stopped.\")\n",
    "            return {\n",
    "                \"filename\": student_filename,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": f\"Low confidence score. Model is not sure about the category. Score: {confidence:.2%}\",\n",
    "                \"classification\": classification,\n",
    "                \"evaluation\": {\"error\": \"Evaluation aborted due to low classification confidence.\"}\n",
    "            }\n",
    "\n",
    "        # Step 2: Get reference solution\n",
    "        reference = self.get_best_reference(category)\n",
    "        if not reference:\n",
    "            return {\n",
    "                \"filename\": student_filename,\n",
    "                \"status\": \"error\", \n",
    "                \"error\": f\"No reference solution for category {category}\",\n",
    "                \"classification\": classification\n",
    "            }\n",
    "        \n",
    "        print(f\"   🎯 Reference: {reference['filename']}\")\n",
    "        \n",
    "        # Step 3: LLM comparison (3-step process)\n",
    "        evaluation = self.compare_with_reference(student_image_path, reference, category)\n",
    "        \n",
    "         # **NEW: LLM-based Quality Gate**\n",
    "        if evaluation.get('qualitaetspruefung', {}).get('is_evaluable') is False:\n",
    "            reason = evaluation.get('qualitaetspruefung', {}).get('reason', 'No reason provided.')\n",
    "            print(f\"   ❌ Evaluation stopped by LLM Quality Gate: {reason}\")\n",
    "            return {\n",
    "                \"filename\": student_filename,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": f\"LLM Quality Gate: Image not evaluable. Reason: {reason}\",\n",
    "                \"classification\": classification,\n",
    "                \"reference_used\": reference,\n",
    "                \"evaluation\": evaluation\n",
    "            }\n",
    "\n",
    "        if \"error\" in evaluation:\n",
    "            print(f\"   ❌ LLM evaluation failed: {evaluation['error']}\")\n",
    "        else:\n",
    "            # Check both new and old format keys\n",
    "            if 'gesamtbewertung' in evaluation:  # New format (single word)\n",
    "                bewertung = evaluation['gesamtbewertung']\n",
    "                score = bewertung.get('gesamtpunktzahl', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                print(f\"   ✅ Score: {score}/100, Grade: {grade}\")\n",
    "            elif 'gesamt_bewertung' in evaluation:  # Old format (with underscore)\n",
    "                bewertung = evaluation['gesamt_bewertung']\n",
    "                score = bewertung.get('gesamtpunkte', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                print(f\"   ✅ Score: {score}/100, Grade: {grade}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Incomplete evaluation response\")\n",
    "                print(f\"   Available keys: {list(evaluation.keys())}\")\n",
    "        \n",
    "        # Compile final result\n",
    "        result = {\n",
    "            \"filename\": student_filename,\n",
    "            \"image_path\": student_image_path,\n",
    "            \"classification\": classification,\n",
    "            \"reference_used\": reference,\n",
    "            \"evaluation\": evaluation,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"status\": \"success\" if \"error\" not in evaluation else \"partial\"\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize evaluation system\n",
    "API_KEY = \"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\"\n",
    "\n",
    "try:\n",
    "    evaluator = Aufgabe4EvaluationSystem(API_KEY)\n",
    "    print(\"\\n🎓 Aufgabe 4 Evaluation System ready!\")\n",
    "    print(f\"📊 Categories with references: {len(evaluator.reference_solutions)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Evaluation system setup failed: {e}\")\n",
    "    evaluator = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Ready to test category-specific prompts!\n",
      "💡 Call test_single_image_evaluation() to run a test\n",
      "🧪 Testing with: 8d281e0d909c4b9c842d967421edae11.png\n",
      "📁 Path: C:\\Users\\egese\\Desktop\\dataset\\val\\SAP\\8d281e0d909c4b9c842d967421edae11.png\n",
      "============================================================\n",
      "🔍 Evaluating: 8d281e0d909c4b9c842d967421edae11.png\n",
      "   📊 Classified as: Excel-Tabelle (70.50%)\n",
      "   🎯 Reference: b3444b458ad24811833429eb7c7dacb8.png\n",
      "   Prüfe Bildqualität: 8d281e0d909c4b9c842d967421edae11.png\n",
      "   ✅ Bildqualität OK: Das Bild zeigt eine klare und lesbare Tabelle mit finanziellen Daten, die für eine Bewertung geeignet sind. Es gibt keine UI-Elemente, die den Inhalt stören, und die Qualität ist ausreichend hoch.\n",
      "   📋 Using category-specific prompt for: Excel-Tabelle\n",
      "   Analysiere Bild: 8d281e0d909c4b9c842d967421edae11.png\n",
      "   Analysiere Bild: b3444b458ad24811833429eb7c7dacb8.png\n",
      "   Vergleiche die beiden Analysen...\n",
      "   📋 Using category-specific prompt for: Excel-Tabelle\n",
      "   ⚠️ Incomplete evaluation response\n",
      "   Available keys: ['struktur_qualitaet', 'visueller_aufbau', 'funktionale_elemente', 'sap_kontext', 'gesamt_score', 'verbesserungsvorschlaege', 'bewertungs_zusammenfassung', 'qualitaetspruefung', 'evaluation_status']\n",
      "\n",
      "🎯 EVALUATION RESULTS:\n",
      "============================================================\n",
      "📊 CNN Classification: Excel-Tabelle (70.50%)\n",
      "🎯 Reference: b3444b458ad24811833429eb7c7dacb8.png\n",
      "✅ Final Score: 75/100\n",
      "📝 Grade: 2.5\n",
      "🎓 Passed: Yes\n",
      "💬 Feedback: Category-specific evaluation for Excel-Tabelle\n",
      "🏷️ Category-Specific Analysis: Excel-Tabelle\n",
      "📊 Analysis Sections: struktur_qualitaet, visueller_aufbau, funktionale_elemente, sap_kontext\n",
      "⚠️ Main Issues: 2 items\n",
      "   1. Fehlende Formeln und Filteroptionen\n",
      "   2. Zu viele Zeilen, was die Übersichtlichkeit beeinträchtigt\n",
      "💡 Recommendations: 3 items\n",
      "   1. Reduzieren der Zeilenanzahl durch Aggregation oder Filterung\n",
      "   2. Einbinden von dynamischen Formeln und Filtern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filename': '8d281e0d909c4b9c842d967421edae11.png',\n",
       " 'image_path': 'C:\\\\Users\\\\egese\\\\Desktop\\\\dataset\\\\val\\\\SAP\\\\8d281e0d909c4b9c842d967421edae11.png',\n",
       " 'classification': {'category': 'Excel-Tabelle',\n",
       "  'confidence': 0.7050408124923706,\n",
       "  'status': 'success'},\n",
       " 'reference_used': {'filename': 'b3444b458ad24811833429eb7c7dacb8.png',\n",
       "  'path': 'C:\\\\Users\\\\egese\\\\Desktop\\\\dataset\\\\mapped_train\\\\Excel-Tabelle\\\\b3444b458ad24811833429eb7c7dacb8.png',\n",
       "  'details': {'image_path': 'C:\\\\Users\\\\egese\\\\Desktop\\\\dataset\\\\mapped_train\\\\Excel-Tabelle\\\\b3444b458ad24811833429eb7c7dacb8.png',\n",
       "   'filename': 'b3444b458ad24811833429eb7c7dacb8.png',\n",
       "   'category': 'Excel-Tabelle',\n",
       "   'predicted_class': 'Excel-Tabelle',\n",
       "   'confidence': 0.9967940449714661,\n",
       "   'category_match': True,\n",
       "   'dimensions': '1920x1080',\n",
       "   'file_size_mb': 0.2,\n",
       "   'format': 'PNG',\n",
       "   'quality_score': 5,\n",
       "   'confidence_score': 9,\n",
       "   'total_score': 7.5,\n",
       "   'evaluation_date': '2025-06-22T20:32:25.434880'},\n",
       "  'category_path': 'Excel-Tabelle'},\n",
       " 'evaluation': {'struktur_qualitaet': {'hat_tabellenstruktur': True,\n",
       "   'spalten_anzahl': '4 (Referenz: 2)',\n",
       "   'zeilen_anzahl': '100+ (Referenz: 10)',\n",
       "   'hat_kopfzeile': True,\n",
       "   'score': 7},\n",
       "  'visueller_aufbau': {'farbschema': 'bunt (Referenz: blau)',\n",
       "   'gridlines_sichtbar': True,\n",
       "   'text_lesbarkeit': 'gut',\n",
       "   'score': 9},\n",
       "  'funktionale_elemente': {'formeln_vorhanden': False,\n",
       "   'summen_berechnung': True,\n",
       "   'diagramme_enthalten': False,\n",
       "   'filter_aktiviert': False,\n",
       "   'score': 5},\n",
       "  'sap_kontext': {'sap_interface_erkennbar': True,\n",
       "   'daten_typ': 'BW Query',\n",
       "   'business_kontext': 'Financial',\n",
       "   'score': 8},\n",
       "  'gesamt_score': 7.5,\n",
       "  'verbesserungsvorschlaege': ['Bessere Formatierung der Zahlen (z.B. Tausendertrennzeichen)',\n",
       "   'Konsistente Farbcodierung für bessere Übersichtlichkeit',\n",
       "   'Hinzufügen von Diagrammen für visuelle Darstellung der Trends',\n",
       "   'Formeln einfügen für dynamische Berechnungen',\n",
       "   'Filteroptionen aktivieren für bessere Datenanalyse'],\n",
       "  'bewertungs_zusammenfassung': {'gesamtpunktzahl': 75,\n",
       "   'note': 2.5,\n",
       "   'bestanden': True,\n",
       "   'hauptkritikpunkte': ['Fehlende Formeln und Filteroptionen',\n",
       "    'Zu viele Zeilen, was die Übersichtlichkeit beeinträchtigt'],\n",
       "   'verbesserungsempfehlungen': ['Reduzieren der Zeilenanzahl durch Aggregation oder Filterung',\n",
       "    'Einbinden von dynamischen Formeln und Filtern',\n",
       "    'Hinzufügen von Diagrammen für bessere visuelle Darstellung']},\n",
       "  'qualitaetspruefung': {'is_evaluable': True,\n",
       "   'reason': 'Das Bild zeigt eine klare und lesbare Tabelle mit finanziellen Daten, die für eine Bewertung geeignet sind. Es gibt keine UI-Elemente, die den Inhalt stören, und die Qualität ist ausreichend hoch.'},\n",
       "  'evaluation_status': 'evaluated'},\n",
       " 'timestamp': '2025-06-24T22:33:08.713424',\n",
       " 'status': 'success'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 🧪 Quick Test - Single Image Evaluation\n",
    "\n",
    "# Test the updated system with category-specific prompts\n",
    "def test_single_image_evaluation():\n",
    "    \"\"\"Test evaluation with a single image\"\"\"\n",
    "    if evaluator is None:\n",
    "        print(\"❌ Evaluator not initialized\")\n",
    "        return\n",
    "    \n",
    "    # Test with a sample image from val/SAP\n",
    "    test_images = [f for f in os.listdir(VAL_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not test_images:\n",
    "        print(\"❌ No test images found in val/SAP\")\n",
    "        return\n",
    "    \n",
    "    # Pick first available image\n",
    "    test_image = \"8d281e0d909c4b9c842d967421edae11.png\"\n",
    "    test_path = os.path.join(VAL_DIR, test_image)\n",
    "    \n",
    "    print(f\"🧪 Testing with: {test_image}\")\n",
    "    print(f\"📁 Path: {test_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluator.evaluate_student_submission(test_path, test_image)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n🎯 EVALUATION RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        classification = result['classification']\n",
    "        print(f\"📊 CNN Classification: {classification['category']} ({classification['confidence']:.2%})\")\n",
    "        \n",
    "        reference = result['reference_used']\n",
    "        print(f\"🎯 Reference: {reference['filename']}\")\n",
    "        \n",
    "        evaluation = result['evaluation']\n",
    "        if 'error' not in evaluation:\n",
    "            # Check for category-specific results first\n",
    "            if 'bewertungs_zusammenfassung' in evaluation:  # New category-specific format\n",
    "                bewertung = evaluation['bewertungs_zusammenfassung']\n",
    "                score = bewertung.get('gesamtpunktzahl', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                passed = bewertung.get('bestanden', False)\n",
    "                feedback = f\"Category-specific evaluation for {classification['category']}\"\n",
    "                \n",
    "                print(f\"✅ Final Score: {score}/100\")\n",
    "                print(f\"📝 Grade: {grade}\")\n",
    "                print(f\"🎓 Passed: {'Yes' if passed else 'No'}\")\n",
    "                print(f\"💬 Feedback: {feedback}\")\n",
    "                \n",
    "                # Show category-specific analysis structure\n",
    "                print(f\"🏷️ Category-Specific Analysis: {classification['category']}\")\n",
    "                \n",
    "                # Show main sections from category-specific analysis\n",
    "                category_sections = [k for k in evaluation.keys() if k not in ['bewertungs_zusammenfassung']]\n",
    "                if category_sections:\n",
    "                    print(f\"📊 Analysis Sections: {', '.join(category_sections[:4])}\")\n",
    "                \n",
    "                # Show criticisms and recommendations\n",
    "                kritik = bewertung.get('hauptkritikpunkte', [])\n",
    "                empfehlungen = bewertung.get('verbesserungsempfehlungen', [])\n",
    "                \n",
    "                if kritik:\n",
    "                    print(f\"⚠️ Main Issues: {len(kritik)} items\")\n",
    "                    for i, issue in enumerate(kritik[:2], 1):\n",
    "                        print(f\"   {i}. {issue}\")\n",
    "                        \n",
    "                if empfehlungen:\n",
    "                    print(f\"💡 Recommendations: {len(empfehlungen)} items\")\n",
    "                    for i, rec in enumerate(empfehlungen[:2], 1):\n",
    "                        print(f\"   {i}. {rec}\")\n",
    "                        \n",
    "            elif 'gesamtbewertung' in evaluation:  # Generic format fallback\n",
    "                bewertung = evaluation['gesamtbewertung']\n",
    "                score = bewertung.get('gesamtpunktzahl', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                passed = bewertung.get('bestanden', False)\n",
    "                feedback = bewertung.get('feedback', 'No feedback')\n",
    "                \n",
    "                print(f\"✅ Final Score: {score}/100\")\n",
    "                print(f\"📝 Grade: {grade}\")\n",
    "                print(f\"🎓 Passed: {'Yes' if passed else 'No'}\")\n",
    "                print(f\"💬 Feedback: {feedback}\")\n",
    "                \n",
    "                # Check for suggestions (both formats)\n",
    "                suggestions = evaluation.get('empfehlungen', evaluation.get('verbesserungsvorschlaege', []))\n",
    "                if suggestions:\n",
    "                    print(f\"💡 Suggestions: {len(suggestions)} items\")\n",
    "                    for i, suggestion in enumerate(suggestions[:3], 1):\n",
    "                        print(f\"   {i}. {suggestion}\")\n",
    "                        \n",
    "                # Show category-specific analysis\n",
    "                if 'kategorie_vergleich' in evaluation:\n",
    "                    kategorie_info = evaluation['kategorie_vergleich']\n",
    "                    print(f\"🏷️ Category: {kategorie_info.get('kategorie', 'Unknown')}\")\n",
    "                    print(f\"📊 Category Match: {kategorie_info.get('student_erfuellt_kategorie', False)}\")\n",
    "                    \n",
    "                # Show detailed scores\n",
    "                if 'detailbewertung' in evaluation:\n",
    "                    detail_scores = evaluation['detailbewertung']\n",
    "                    print(\"📈 Detailed Scores:\")\n",
    "                    print(f\"   Struktur: {detail_scores.get('struktur_score', 0)}/100\")\n",
    "                    print(f\"   Technik: {detail_scores.get('technik_score', 0)}/100\") \n",
    "                    print(f\"   Vollständigkeit: {detail_scores.get('vollstaendigkeit_score', 0)}/100\")\n",
    "                    print(f\"   Korrektheit: {detail_scores.get('korrektheit_score', 0)}/100\")\n",
    "                    \n",
    "            elif 'gesamt_bewertung' in evaluation:  # Old format (with underscore)\n",
    "                bewertung = evaluation['gesamt_bewertung']\n",
    "                score = bewertung.get('gesamtpunkte', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                passed = bewertung.get('bestanden', False)\n",
    "                feedback = bewertung.get('feedback', 'No feedback')\n",
    "                \n",
    "                print(f\"✅ Final Score: {score}/100\")\n",
    "                print(f\"📝 Grade: {grade}\")\n",
    "                print(f\"🎓 Passed: {'Yes' if passed else 'No'}\")\n",
    "                print(f\"💬 Feedback: {feedback}\")\n",
    "            else:\n",
    "                print(\"⚠️ Unexpected response format\")\n",
    "                print(\"Raw evaluation keys:\", list(evaluation.keys()))\n",
    "                # Try to extract any available scores\n",
    "                if 'detailbewertung' in evaluation:\n",
    "                    detail_scores = evaluation['detailbewertung']\n",
    "                    avg_score = sum(detail_scores.values()) / len(detail_scores) if detail_scores else 0\n",
    "                    print(f\"📊 Average Score: {avg_score:.1f}/100\")\n",
    "        else:\n",
    "            print(f\"❌ Evaluation error: {evaluation['error']}\")\n",
    "    else:\n",
    "        print(f\"❌ Evaluation failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Ready to test!\n",
    "print(\"🧪 Ready to test category-specific prompts!\")\n",
    "print(\"💡 Call test_single_image_evaluation() to run a test\")\n",
    "test_single_image_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
