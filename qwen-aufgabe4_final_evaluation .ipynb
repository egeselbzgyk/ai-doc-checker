{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üéì Aufgabe 4 - Final Evaluation System\n",
    "\n",
    "**Vergleich mit Musterl√∂sung - Complete System Test**\n",
    "\n",
    "Bu notebook **Aufgabe 4**'√ºn final implementation'ƒ±:\n",
    "\n",
    "1. **Complete Workflow Test** - Ende-zu-Ende sistem testi\n",
    "2. **Student vs Reference** kar≈üƒ±la≈ütƒ±rmasƒ±  \n",
    "3. **Automated Grading** sistemi\n",
    "4. **Results Export** - PDF raporlar\n",
    "5. **Performance Analysis** - sistem deƒüerlendirmesi\n",
    "\n",
    "## üèÜ Das ist der finale Schritt f√ºr Aufgabe 4!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Aufgabe 4 Final Evaluation System\n",
      "üéØ Ready to test complete workflow!\n",
      "üìÅ Validation images: C:\\Users\\egese\\Desktop\\dataset\\val\\SAP\n",
      "üìä Analysis results: results/validation_analysis/\n",
      "üéØ Reference solutions: results/reference_solutions/\n",
      "üèÜ Final output: results/aufgabe4_final/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Local imports\n",
    "from predict import predict_image\n",
    "from prompt_templates import prompt_templates, comparison_prompt\n",
    "\n",
    "print(\"‚úÖ Aufgabe 4 Final Evaluation System\")\n",
    "print(\"üéØ Ready to test complete workflow!\")\n",
    "\n",
    "# Setup paths\n",
    "VAL_DIR = r\"C:\\Users\\egese\\Desktop\\dataset\\val\\SAP\"          # Student solutions (only for the test)\n",
    "RESULTS_DIR = \"results/validation_analysis/\"\n",
    "REFERENCE_DIR = \"results/reference_solutions/\"\n",
    "FINAL_OUTPUT = \"results/aufgabe4_final/\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(FINAL_OUTPUT, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Validation images: {VAL_DIR}\")\n",
    "print(f\"üìä Analysis results: {RESULTS_DIR}\")\n",
    "print(f\"üéØ Reference solutions: {REFERENCE_DIR}\")\n",
    "print(f\"üèÜ Final output: {FINAL_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Complete Evaluation Class\n",
    "\n",
    "Dies ist die finale Implementation von Aufgabe 4:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API ready for evaluation (Model: qwen/qwen2.5-vl-32b-instruct)\n",
      "üîß Using Qwen2.5-VL-32B for improved JSON processing\n",
      "‚úÖ Found reference in directory: reference_database_latest.json\n",
      "‚úÖ Loaded reference database: 6 categories\n",
      "   üìÅ Data Source: 5 reference(s)\n",
      "   üìÅ Data-Flow: 5 reference(s)\n",
      "   üìÅ Data-Transfer-Process: 5 reference(s)\n",
      "   üìÅ Excel-Tabelle: 5 reference(s)\n",
      "   üìÅ Info-Object: 5 reference(s)\n",
      "   üìÅ Transformation: 5 reference(s)\n",
      "\n",
      "üéì Aufgabe 4 Evaluation System ready!\n",
      "üìä Categories with references: 6\n"
     ]
    }
   ],
   "source": [
    "class Aufgabe4EvaluationSystem:\n",
    "    def __init__(self, api_key):\n",
    "        \"\"\"Complete evaluation system for Aufgabe 4\"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.setup_api()\n",
    "        \n",
    "        # Load reference solutions\n",
    "        self.reference_solutions = {}\n",
    "        self.load_reference_database()\n",
    "        \n",
    "        # Evaluation results\n",
    "        self.evaluation_results = []\n",
    "        \n",
    "    def setup_api(self):\n",
    "        \"\"\"Setup OpenRouter API\"\"\"\n",
    "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        # Use more reliable model for complex multi-image tasks\n",
    "        self.model = \"qwen/qwen2.5-vl-32b-instruct\" # Better multi-image support\n",
    "        # Alternative models if current fails:\n",
    "        # self.model = \"qwen/qwen-vl-max\" # Backup model 1\n",
    "        # self.model = \"anthropic/claude-3-sonnet:beta\" # Backup model 2\n",
    "        print(f\"‚úÖ API ready for evaluation (Model: {self.model})\")\n",
    "        print(\"üîß Using Qwen2.5-VL-32B for improved JSON processing\")\n",
    "    \n",
    "    def switch_model(self, model_name):\n",
    "        \"\"\"Switch to a different model if current one has issues\"\"\"\n",
    "        old_model = self.model\n",
    "        self.model = model_name\n",
    "        print(f\"üîÑ Switched model: {old_model} ‚Üí {model_name}\")\n",
    "    \n",
    "    def load_reference_database(self):\n",
    "        \"\"\"Load reference solutions from JSON files\"\"\"\n",
    "        # Try multiple possible locations for reference database\n",
    "        possible_paths = [\n",
    "            \"reference_database_latest.json\",\n",
    "            \"reference_database_auto_*.json\", \n",
    "            os.path.join(REFERENCE_DIR, \"reference_solutions_*.json\"),\n",
    "            os.path.join(REFERENCE_DIR, \"reference_database_*.json\")\n",
    "        ]\n",
    "        \n",
    "        reference_file = None\n",
    "        \n",
    "        # Check for latest reference database in current directory first\n",
    "        if os.path.exists(\"reference_database_latest.json\"):\n",
    "            reference_file = \"reference_database_latest.json\"\n",
    "            print(\"‚úÖ Found reference_database_latest.json\")\n",
    "        else:\n",
    "            # Look for auto-generated files\n",
    "            import glob\n",
    "            auto_files = glob.glob(\"reference_database_auto_*.json\")\n",
    "            if auto_files:\n",
    "                reference_file = sorted(auto_files)[-1]  # Get latest\n",
    "                print(f\"‚úÖ Found auto-generated reference: {reference_file}\")\n",
    "            else:\n",
    "                # Check reference directory if it exists\n",
    "                if os.path.exists(REFERENCE_DIR):\n",
    "                    ref_files = [f for f in os.listdir(REFERENCE_DIR) if f.startswith('reference_')]\n",
    "                    if ref_files:\n",
    "                        latest_ref = sorted(ref_files)[-1]\n",
    "                        reference_file = os.path.join(REFERENCE_DIR, latest_ref)\n",
    "                        print(f\"‚úÖ Found reference in directory: {latest_ref}\")\n",
    "        \n",
    "        if reference_file and os.path.exists(reference_file):\n",
    "            try:\n",
    "                with open(reference_file, 'r', encoding='utf-8') as f:\n",
    "                    ref_data = json.load(f)\n",
    "                    \n",
    "                # Handle different JSON structures\n",
    "                if 'references' in ref_data:\n",
    "                    self.reference_solutions = ref_data['references']\n",
    "                elif 'categories' in ref_data:\n",
    "                    self.reference_solutions = ref_data['categories']\n",
    "                else:\n",
    "                    # Assume the data itself is the reference structure\n",
    "                    self.reference_solutions = ref_data\n",
    "                \n",
    "                print(f\"‚úÖ Loaded reference database: {len(self.reference_solutions)} categories\")\n",
    "                for category, refs in self.reference_solutions.items():\n",
    "                    if isinstance(refs, list):\n",
    "                        print(f\"   üìÅ {category}: {len(refs)} reference(s)\")\n",
    "                    else:\n",
    "                        print(f\"   üìÅ {category}: {type(refs)} data\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading reference database: {e}\")\n",
    "                self.reference_solutions = {}\n",
    "        else:\n",
    "            print(\"‚ùå No reference database found!\")\n",
    "            print(\"üí° Available files:\", [f for f in os.listdir('.') if 'reference' in f])\n",
    "            self.reference_solutions = {}\n",
    "    \n",
    "    def classify_student_submission(self, image_path):\n",
    "        \"\"\"CNN classification of student submission\"\"\"\n",
    "        try:\n",
    "            predicted_class, confidence = predict_image(image_path)\n",
    "            return {\n",
    "                \"category\": predicted_class,\n",
    "                \"confidence\": float(confidence),\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"category\": None,\n",
    "                \"confidence\": 0.0,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def get_best_reference(self, category):\n",
    "        \"\"\"Get best reference for a category\"\"\"\n",
    "        if category not in self.reference_solutions:\n",
    "            return None\n",
    "        \n",
    "        references = self.reference_solutions[category]\n",
    "        if not references:\n",
    "            return None\n",
    "        \n",
    "        # Get first (best) reference\n",
    "        best_ref = references[0]\n",
    "        ref_filename = best_ref['filename']\n",
    "        \n",
    "        # Reference images are in mapped_train, organized by category\n",
    "        # Try different possible paths\n",
    "        possible_paths = [\n",
    "            os.path.join(r\"C:\\Users\\egese\\Desktop\\dataset\\mapped_train\", category, ref_filename),\n",
    "            os.path.join(r\"C:\\Users\\egese\\Desktop\\dataset\\mapped_train\", category.replace(\" \", \"-\"), ref_filename),\n",
    "            os.path.join(r\"C:\\Users\\egese\\Desktop\\dataset\\mapped_train\", category.replace(\"-\", \" \"), ref_filename),\n",
    "            os.path.join(VAL_DIR, ref_filename),  # Fallback to val directory\n",
    "        ]\n",
    "        \n",
    "        for ref_path in possible_paths:\n",
    "            if os.path.exists(ref_path):\n",
    "                return {\n",
    "                    \"filename\": ref_filename,\n",
    "                    \"path\": ref_path,\n",
    "                    \"details\": best_ref,\n",
    "                    \"category_path\": category\n",
    "                }\n",
    "        \n",
    "        print(f\"‚ö†Ô∏è Reference image not found: {ref_filename} for category {category}\")\n",
    "        print(f\"   Tried paths: {possible_paths}\")\n",
    "        return None\n",
    "    \n",
    "    def get_image_media_type(self, image_path):\n",
    "        \"\"\"Detect actual image format and return correct media type\"\"\"\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                format_name = img.format.lower()\n",
    "                if format_name == 'jpeg':\n",
    "                    return 'image/jpeg'\n",
    "                elif format_name == 'png':\n",
    "                    return 'image/png'\n",
    "                elif format_name == 'gif':\n",
    "                    return 'image/gif'\n",
    "                elif format_name == 'webp':\n",
    "                    return 'image/webp'\n",
    "                else:\n",
    "                    return 'image/jpeg'  # Default fallback\n",
    "        except Exception:\n",
    "            # Fallback based on file extension\n",
    "            ext = os.path.splitext(image_path)[1].lower()\n",
    "            if ext in ['.png']:\n",
    "                return 'image/png'\n",
    "            elif ext in ['.gif']:\n",
    "                return 'image/gif'\n",
    "            elif ext in ['.webp']:\n",
    "                return 'image/webp'\n",
    "            else:\n",
    "                return 'image/jpeg'  # Default\n",
    "    \n",
    "    def encode_image(self, image_path):\n",
    "        \"\"\"Encode image to base64 with correct media type\"\"\"\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                base64_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "                media_type = self.get_image_media_type(image_path)\n",
    "                return base64_data, media_type\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Image encoding error: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def get_category_prompt(self, category):\n",
    "        \"\"\"Get category-specific prompt from prompt_templates.py\"\"\"\n",
    "        # Mapping zwischen CNN predictions und prompt_templates keys\n",
    "        category_mapping = {\n",
    "            \"Excel-Tabelle\": \"Excel-Tabelle\",\n",
    "            \"Data-Flow\": \"Data-Flow\", \n",
    "            \"Data-Transfer-Process\": \"Data-Transfer-Process\",\n",
    "            \"Transformation\": \"Transformation\",\n",
    "            \"Data Source\": \"Data Source\",\n",
    "            \"Info-Object\": \"Info-Object\",\n",
    "            # Alternative Namensgebungen falls n√∂tig\n",
    "            \"Data-Transfer\": \"Data-Transfer-Process\",\n",
    "            \"DataFlow\": \"Data-Flow\",\n",
    "            \"DataSource\": \"Data Source\",\n",
    "            \"InfoObject\": \"Info-Object\",\n",
    "            \"Excel\": \"Excel-Tabelle\"\n",
    "        }\n",
    "        \n",
    "        # Richtige Kategorie finden\n",
    "        mapped_category = category_mapping.get(category, category)\n",
    "        \n",
    "        if mapped_category in prompt_templates:\n",
    "            print(f\"   üìã Using category-specific prompt for: {mapped_category}\")\n",
    "            return prompt_templates[mapped_category]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No specific prompt for category '{category}', using generic prompt\")\n",
    "            return self.get_generic_prompt()\n",
    "    \n",
    "    def get_generic_prompt(self):\n",
    "        \"\"\"Generic prompt if category-specific not found\"\"\"\n",
    "        return \"\"\"\n",
    "Analysiere das Bild und bewerte nach folgenden Kriterien:\n",
    "{\n",
    "  \"struktur_qualitaet\": {\n",
    "    \"aufbau_logisch\": true/false,\n",
    "    \"elemente_erkennbar\": true/false,\n",
    "    \"vollstaendigkeit\": 0-10,\n",
    "    \"score\": 0-10\n",
    "  },\n",
    "  \"technische_qualitaet\": {\n",
    "    \"lesbarkeit\": \"gut/mittel/schlecht\",\n",
    "    \"detailgrad\": \"zu wenig/angemessen/zu viel\", \n",
    "    \"fachliche_korrektheit\": 0-10,\n",
    "    \"score\": 0-10\n",
    "  },\n",
    "  \"sap_kontext\": {\n",
    "    \"sap_bw_relevant\": true/false,\n",
    "    \"terminologie_korrekt\": true/false,\n",
    "    \"business_kontext\": \"erkennbar/unklar\",\n",
    "    \"score\": 0-10\n",
    "  },\n",
    "  \"gesamt_score\": 0-10,\n",
    "  \"verbesserungsvorschlaege\": [\"Konkrete Hinweise\"]\n",
    "}\n",
    "\"\"\",\n",
    "    \n",
    "    def _make_api_call(self, messages, max_tokens=2048):\n",
    "        \"\"\"Helper function to make a generic API call.\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"http://localhost:8888\", # Or your app's URL\n",
    "            \"X-Title\": \"Aufgabe 4 Evaluation\"\n",
    "        }\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": 0.1,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(self.base_url, headers=headers, json=data, timeout=120)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "                return {\"response\": content}\n",
    "            else:\n",
    "                return {\"error\": f\"API Error HTTP {response.status_code}: {response.text}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Request Exception: {str(e)}\"}\n",
    "\n",
    "    def _check_image_quality(self, image_path):\n",
    "        \"\"\"Check if image is suitable for evaluation (Schritt 1)\"\"\"\n",
    "        print(f\"   Pr√ºfe Bildqualit√§t: {os.path.basename(image_path)}\")\n",
    "        base64_image, media_type = self.encode_image(image_path)\n",
    "        if not base64_image:\n",
    "            return {\"error\": \"Failed to encode image\"}\n",
    "\n",
    "        prompt_text = \"\"\"SCHRITT 1: PR√úFUNG DER EIGNUNG ZUR BEWERTUNG\n",
    "\n",
    "Pr√ºfe, ob das Studenten-Bild f√ºr eine Bewertung GEEIGNET ist. \n",
    "Ein Bild ist **NICHT GEEIGNET** (`\"is_evaluable\": false`), wenn einer der folgenden Punkte zutrifft:\n",
    "\n",
    "**Fokus auf UI-Elemente statt Inhalt:**\n",
    "- Das Bild zeigt haupts√§chlich ein Men√º (Rechtsklick), Dropdown-Liste, Dialogbox oder Popup-Fehlermeldung\n",
    "- Das Bild zeigt den Login-Screen, SAP-Startseite (Easy Access) oder generische Transaktionsauswahl\n",
    "\n",
    "**Qualit√§t und Lesbarkeit:**\n",
    "- Das Bild ist stark verpixelt, unscharf oder niedrig aufgel√∂st\n",
    "- Das Bild ist extrem dunkel, √ºberbelichtet oder hat geringen Kontrast\n",
    "\n",
    "**Falscher Bildausschnitt:**\n",
    "- Das Bild ist zu stark herangezoomt (nur winziges Detail sichtbar)\n",
    "- Das Bild ist zu weit herausgezoomt (SAP-Fenster sehr klein im Screenshot)\n",
    "\n",
    "**Irrelevanter Inhalt:**\n",
    "- Das Bild zeigt ABAP-Code statt grafisches Modell\n",
    "- Das Bild zeigt andere Anwendung (Windows Explorer, etc.)\n",
    "- Das Bild zeigt leeren/Lade-Bildschirm oder ist unvollst√§ndig\n",
    "- Es gibt irrelevante Inhalte auf dem Bildschirm\n",
    "\n",
    "Gib NUR folgendes JSON zur√ºck:\n",
    "\n",
    "{\n",
    "  \"is_evaluable\": true/false,\n",
    "  \"reason\": \"Kurze Begr√ºndung warum geeignet/nicht geeignet\"\n",
    "}\n",
    "\n",
    "WICHTIG: Antworte NUR mit JSON - keine Markdown-Bl√∂cke, keine Erkl√§rung.\"\"\"\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{media_type};base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": prompt_text}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        quality_result = self._make_api_call(messages)\n",
    "        return self.parse_evaluation_response(quality_result)\n",
    "\n",
    "    def _analyze_single_image(self, image_path, category_prompt):\n",
    "        \"\"\"Analyzes a single image using the LLM and returns the JSON analysis.\"\"\"\n",
    "        print(f\"   Analysiere Bild: {os.path.basename(image_path)}\")\n",
    "        base64_image, media_type = self.encode_image(image_path)\n",
    "        if not base64_image:\n",
    "            return {\"error\": \"Failed to encode image\"}\n",
    "\n",
    "        prompt_text = f\"\"\"Du bist ein SAP BW Experte. Analysiere das Bild detailliert und gib deine Bewertung als JSON zur√ºck.\n",
    "\n",
    "BEWERTUNGSSCHEMA (exakt in diesem Format antworten):\n",
    "{category_prompt}\n",
    "\n",
    "WICHTIGE REGELN:\n",
    "- Verwende EXAKT die Felder aus dem Schema oben\n",
    "- Alle score-Werte sind zwischen 0-10\n",
    "- Alle true/false Werte sind boolean\n",
    "- Alle Textwerte sind Strings in Anf√ºhrungszeichen\n",
    "- Gib NUR das JSON zur√ºck - keine Markdown-Bl√∂cke, keine Erkl√§rung\n",
    "\n",
    "Antworte NUR mit dem JSON-Objekt:\"\"\"\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{media_type};base64,{base64_image}\"}},\n",
    "                {\"type\": \"text\", \"text\": prompt_text}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        analysis_result = self._make_api_call(messages)\n",
    "        return self.parse_evaluation_response(analysis_result)\n",
    "\n",
    "    def debug_single_image_passthrough(self, image_path):\n",
    "        \"\"\"A simple test to see if the model can see ONE image.\"\"\"\n",
    "        print(\"\\n--- üïµÔ∏è RUNNING SINGLE IMAGE DEBUG TEST ---\")\n",
    "        student_b64, student_media_type = self.encode_image(image_path)\n",
    "        if not student_b64:\n",
    "            return {\"error\": \"Failed to encode image\"}\n",
    "\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{student_media_type};base64,{student_b64}\"}},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail in English. What do you see?\"}\n",
    "            ]\n",
    "        }]\n",
    "        \n",
    "        try:\n",
    "            print(\"   ...Sending request to model...\")\n",
    "            result = self._make_api_call(messages, max_tokens=1024)\n",
    "            \n",
    "            if \"error\" not in result:\n",
    "                print(\"\\n--- ‚úÖ Model Response ---\")\n",
    "                print(result[\"response\"])\n",
    "                return result\n",
    "            else:\n",
    "                print(\"\\n--- ‚ùå Error Response ---\")\n",
    "                print(result[\"error\"])\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Request failed: {str(e)}\"}\n",
    "\n",
    "\n",
    "    \n",
    "    def compare_with_reference(self, student_path, reference_info, category):\n",
    "        \"\"\"Compare student submission with reference using 4-step approach with quality check\"\"\"\n",
    "        \n",
    "        # STEP 0: Check if student image is evaluable (quality control)\n",
    "        quality_check = self._check_image_quality(student_path)\n",
    "        if \"error\" in quality_check:\n",
    "            return quality_check\n",
    "            \n",
    "        # If image is not suitable for evaluation, return early with detailed reason\n",
    "        if not quality_check.get(\"is_evaluable\", False):\n",
    "            return {\n",
    "                \"qualitaetspruefung\": {\n",
    "                    \"is_evaluable\": False,\n",
    "                    \"reason\": quality_check.get('reason', 'Bild nicht f√ºr Bewertung geeignet')\n",
    "                },\n",
    "                \"score\": 0,\n",
    "                \"detailed_scores\": {\n",
    "                    \"structure\": 0,\n",
    "                    \"technical_quality\": 0, \n",
    "                    \"completeness\": 0,\n",
    "                    \"correctness\": 0\n",
    "                },\n",
    "                \"strengths\": [],\n",
    "                \"weaknesses\": [\"Bild ist nicht f√ºr Bewertung geeignet\"],\n",
    "                \"suggestions\": [\"Bitte ein aussagekr√§ftiges Screenshot der SAP BW Modellierung einreichen\"],\n",
    "                \"category_specific_feedback\": f\"Nicht bewertbar: {quality_check.get('reason', 'Unbekannter Grund')}\",\n",
    "                \"evaluation_status\": \"not_evaluable\"\n",
    "            }\n",
    "        \n",
    "        print(f\"   ‚úÖ Bildqualit√§t OK: {quality_check.get('reason', '')}\")\n",
    "        \n",
    "        # STEP 1: Analyze student image\n",
    "        category_prompt = self.get_category_prompt(category)\n",
    "        student_analysis = self._analyze_single_image(student_path, category_prompt)\n",
    "        if \"error\" in student_analysis:\n",
    "            return {\"error\": f\"Student image analysis failed: {student_analysis['error']}\"}\n",
    "\n",
    "        # STEP 2: Analyze reference image  \n",
    "        reference_analysis = self._analyze_single_image(reference_info['path'], category_prompt)\n",
    "        if \"error\" in reference_analysis:\n",
    "            return {\"error\": f\"Reference image analysis failed: {reference_analysis['error']}\"}\n",
    "        \n",
    "        # STEP 3: Compare the two analyses using SAME category-specific format\n",
    "        print(\"   Vergleiche die beiden Analysen...\")\n",
    "        \n",
    "        # Get the category-specific template for comparison format\n",
    "        comparison_template = self.get_category_prompt(category)\n",
    "        \n",
    "        comparison_prompt_template = \"\"\"Du bist ein SAP BW Experte. Vergleiche die beiden kategorie-spezifischen Bildanalysen der Kategorie \"{category}\".\n",
    "\n",
    "=== STUDENT ANALYSE ===\n",
    "{student_analysis}\n",
    "\n",
    "=== REFERENZ ANALYSE (Musterl√∂sung) ===\n",
    "{reference_analysis}\n",
    "\n",
    "AUFGABE:\n",
    "1. Vergleiche die beiden Analysen Punkt f√ºr Punkt\n",
    "2. Bewerte jeden Aspekt basierend auf der Referenz  \n",
    "3. Gib das Ergebnis im GLEICHEN Format wie die urspr√ºnglichen Analysen zur√ºck\n",
    "\n",
    "WICHTIG: Verwende EXAKT das gleiche JSON-Schema wie die Einzelanalysen:\n",
    "{comparison_template}\n",
    "\n",
    "Aber ersetze die Werte mit BEWERTUNGEN statt Analyseergebnissen:\n",
    "- F√ºr score-Felder: Gib Punkte 0-10 basierend auf Vergleich mit Referenz\n",
    "- F√ºr boolean-Felder: true wenn Student gut abschneidet, false wenn schlecht\n",
    "- F√ºr Text-Felder: Bewertungskommentare statt Analyseergebnisse\n",
    "- F√ºr Array-Felder: Konkrete Verbesserungsvorschl√§ge\n",
    "\n",
    "Zus√§tzlich f√ºge am Ende hinzu:\n",
    "\"bewertungs_zusammenfassung\": {{\n",
    "  \"gesamtpunktzahl\": <0-100>,\n",
    "  \"note\": <1.0-5.0>,\n",
    "  \"bestanden\": true/false,\n",
    "  \"hauptkritikpunkte\": [\"Punkt 1\", \"Punkt 2\"],\n",
    "  \"verbesserungsempfehlungen\": [\"Empfehlung 1\", \"Empfehlung 2\"]\n",
    "}}\n",
    "\n",
    "Antworte NUR mit dem JSON-Objekt (keine Markdown-Bl√∂cke):\"\"\"\n",
    "\n",
    "        comparison_prompt = comparison_prompt_template.format(\n",
    "            student_analysis=json.dumps(student_analysis, indent=2),\n",
    "            reference_analysis=json.dumps(reference_analysis, indent=2),\n",
    "            category=category,\n",
    "            comparison_template=comparison_template\n",
    "        )\n",
    "        \n",
    "        messages = [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [{\"type\": \"text\", \"text\": comparison_prompt}]\n",
    "        }]\n",
    "        \n",
    "        comparison_result = self._make_api_call(messages)\n",
    "        parsed_result = self.parse_evaluation_response(comparison_result)\n",
    "        \n",
    "        # Add quality check info to result\n",
    "        if isinstance(parsed_result, dict) and \"error\" not in parsed_result:\n",
    "            parsed_result[\"qualitaetspruefung\"] = quality_check\n",
    "            parsed_result[\"evaluation_status\"] = \"evaluated\"\n",
    "            \n",
    "        return parsed_result\n",
    "    \n",
    "    def parse_evaluation_response(self, response):\n",
    "        \"\"\"Parse LLM evaluation response to JSON\"\"\"\n",
    "        if \"error\" in response:\n",
    "            return response\n",
    "        \n",
    "        text_output = response.get('response', '')\n",
    "        \n",
    "        # Clean up common LLM formatting issues\n",
    "        text_output = text_output.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if text_output.startswith('```json'):\n",
    "            text_output = text_output[7:]  # Remove ```json\n",
    "        if text_output.startswith('```'):\n",
    "            text_output = text_output[3:]   # Remove ```\n",
    "        if text_output.endswith('```'):\n",
    "            text_output = text_output[:-3]  # Remove trailing ```\n",
    "        \n",
    "        # Find the JSON object\n",
    "        start_idx = text_output.find('{')\n",
    "        \n",
    "        if start_idx != -1:\n",
    "            # Find the matching closing brace\n",
    "            brace_count = 0\n",
    "            end_idx = start_idx\n",
    "            for i, char in enumerate(text_output[start_idx:], start_idx):\n",
    "                if char == '{':\n",
    "                    brace_count += 1\n",
    "                elif char == '}':\n",
    "                    brace_count -= 1\n",
    "                    if brace_count == 0:\n",
    "                        end_idx = i + 1\n",
    "                        break\n",
    "            \n",
    "            if brace_count == 0:  # Found matching closing brace\n",
    "                json_str = text_output[start_idx:end_idx]\n",
    "                try:\n",
    "                    parsed = json.loads(json_str)\n",
    "                    return parsed\n",
    "                except json.JSONDecodeError as e:\n",
    "                    return {\n",
    "                        \"error\": f\"JSON parse error: {str(e)}\",\n",
    "                        \"raw_response\": json_str[:500],\n",
    "                        \"full_response\": text_output[:1000]\n",
    "                    }\n",
    "        \n",
    "        return {\n",
    "            \"error\": \"Valid JSON not found in response\",\n",
    "            \"raw_text\": text_output[:500]\n",
    "        }\n",
    "    \n",
    "    def evaluate_student_submission(self, student_image_path, student_filename):\n",
    "        \"\"\"Complete evaluation of a single student submission\"\"\"\n",
    "        print(f\"üîç Evaluating: {student_filename}\")\n",
    "        \n",
    "        # Define a confidence threshold\n",
    "        CONFIDENCE_THRESHOLD = 0.60\n",
    "        \n",
    "        # Step 1: CNN Classification\n",
    "        classification = self.classify_student_submission(student_image_path)\n",
    "        if classification['status'] != 'success':\n",
    "            return {\n",
    "                \"filename\": student_filename,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"CNN classification failed\",\n",
    "                \"classification\": classification\n",
    "            }\n",
    "        \n",
    "        category = classification['category']\n",
    "        confidence = classification['confidence']\n",
    "        print(f\"   üìä Classified as: {category} ({confidence:.2%})\")\n",
    "        \n",
    "        # **NEW: Quality Gate based on Confidence Score**\n",
    "        if confidence < CONFIDENCE_THRESHOLD:\n",
    "            print(f\"   ‚ö†Ô∏è Low confidence score ({confidence:.2%}). Evaluation stopped.\")\n",
    "            return {\n",
    "                \"filename\": student_filename,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": f\"Low confidence score. Model is not sure about the category. Score: {confidence:.2%}\",\n",
    "                \"classification\": classification,\n",
    "                \"evaluation\": {\"error\": \"Evaluation aborted due to low classification confidence.\"}\n",
    "            }\n",
    "\n",
    "        # Step 2: Get reference solution\n",
    "        reference = self.get_best_reference(category)\n",
    "        if not reference:\n",
    "            return {\n",
    "                \"filename\": student_filename,\n",
    "                \"status\": \"error\", \n",
    "                \"error\": f\"No reference solution for category {category}\",\n",
    "                \"classification\": classification\n",
    "            }\n",
    "        \n",
    "        print(f\"   üéØ Reference: {reference['filename']}\")\n",
    "        \n",
    "        # Step 3: LLM comparison (3-step process)\n",
    "        evaluation = self.compare_with_reference(student_image_path, reference, category)\n",
    "        \n",
    "         # **NEW: LLM-based Quality Gate**\n",
    "        if evaluation.get('qualitaetspruefung', {}).get('is_evaluable') is False:\n",
    "            reason = evaluation.get('qualitaetspruefung', {}).get('reason', 'No reason provided.')\n",
    "            print(f\"   ‚ùå Evaluation stopped by LLM Quality Gate: {reason}\")\n",
    "            return {\n",
    "                \"filename\": student_filename,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": f\"LLM Quality Gate: Image not evaluable. Reason: {reason}\",\n",
    "                \"classification\": classification,\n",
    "                \"reference_used\": reference,\n",
    "                \"evaluation\": evaluation\n",
    "            }\n",
    "\n",
    "        if \"error\" in evaluation:\n",
    "            print(f\"   ‚ùå LLM evaluation failed: {evaluation['error']}\")\n",
    "        else:\n",
    "            # Check both new and old format keys\n",
    "            if 'gesamtbewertung' in evaluation:  # New format (single word)\n",
    "                bewertung = evaluation['gesamtbewertung']\n",
    "                score = bewertung.get('gesamtpunktzahl', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                print(f\"   ‚úÖ Score: {score}/100, Grade: {grade}\")\n",
    "            elif 'gesamt_bewertung' in evaluation:  # Old format (with underscore)\n",
    "                bewertung = evaluation['gesamt_bewertung']\n",
    "                score = bewertung.get('gesamtpunkte', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                print(f\"   ‚úÖ Score: {score}/100, Grade: {grade}\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Incomplete evaluation response\")\n",
    "                print(f\"   Available keys: {list(evaluation.keys())}\")\n",
    "        \n",
    "        # Compile final result\n",
    "        result = {\n",
    "            \"filename\": student_filename,\n",
    "            \"image_path\": student_image_path,\n",
    "            \"classification\": classification,\n",
    "            \"reference_used\": reference,\n",
    "            \"evaluation\": evaluation,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"status\": \"success\" if \"error\" not in evaluation else \"partial\"\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize evaluation system\n",
    "API_KEY = \"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\"\n",
    "\n",
    "try:\n",
    "    evaluator = Aufgabe4EvaluationSystem(API_KEY)\n",
    "    print(\"\\nüéì Aufgabe 4 Evaluation System ready!\")\n",
    "    print(f\"üìä Categories with references: {len(evaluator.reference_solutions)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation system setup failed: {e}\")\n",
    "    evaluator = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Ready to test category-specific prompts!\n",
      "üí° Call test_single_image_evaluation() to run a test\n",
      "üß™ Testing with: 8d281e0d909c4b9c842d967421edae11.png\n",
      "üìÅ Path: C:\\Users\\egese\\Desktop\\dataset\\val\\SAP\\8d281e0d909c4b9c842d967421edae11.png\n",
      "============================================================\n",
      "üîç Evaluating: 8d281e0d909c4b9c842d967421edae11.png\n",
      "   üìä Classified as: Excel-Tabelle (70.50%)\n",
      "   üéØ Reference: b3444b458ad24811833429eb7c7dacb8.png\n",
      "   Pr√ºfe Bildqualit√§t: 8d281e0d909c4b9c842d967421edae11.png\n",
      "   ‚úÖ Bildqualit√§t OK: Das Bild zeigt eine klare und lesbare Tabelle mit finanziellen Daten, die f√ºr eine Bewertung geeignet sind. Es gibt keine UI-Elemente, die den Inhalt st√∂ren, und die Qualit√§t ist ausreichend hoch.\n",
      "   üìã Using category-specific prompt for: Excel-Tabelle\n",
      "   Analysiere Bild: 8d281e0d909c4b9c842d967421edae11.png\n",
      "   Analysiere Bild: b3444b458ad24811833429eb7c7dacb8.png\n",
      "   Vergleiche die beiden Analysen...\n",
      "   üìã Using category-specific prompt for: Excel-Tabelle\n",
      "   ‚ö†Ô∏è Incomplete evaluation response\n",
      "   Available keys: ['struktur_qualitaet', 'visueller_aufbau', 'funktionale_elemente', 'sap_kontext', 'gesamt_score', 'verbesserungsvorschlaege', 'bewertungs_zusammenfassung', 'qualitaetspruefung', 'evaluation_status']\n",
      "\n",
      "üéØ EVALUATION RESULTS:\n",
      "============================================================\n",
      "üìä CNN Classification: Excel-Tabelle (70.50%)\n",
      "üéØ Reference: b3444b458ad24811833429eb7c7dacb8.png\n",
      "‚úÖ Final Score: 75/100\n",
      "üìù Grade: 2.5\n",
      "üéì Passed: Yes\n",
      "üí¨ Feedback: Category-specific evaluation for Excel-Tabelle\n",
      "üè∑Ô∏è Category-Specific Analysis: Excel-Tabelle\n",
      "üìä Analysis Sections: struktur_qualitaet, visueller_aufbau, funktionale_elemente, sap_kontext\n",
      "‚ö†Ô∏è Main Issues: 2 items\n",
      "   1. Fehlende Formeln und Filteroptionen\n",
      "   2. Zu viele Zeilen, was die √úbersichtlichkeit beeintr√§chtigt\n",
      "üí° Recommendations: 3 items\n",
      "   1. Reduzieren der Zeilenanzahl durch Aggregation oder Filterung\n",
      "   2. Einbinden von dynamischen Formeln und Filtern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'filename': '8d281e0d909c4b9c842d967421edae11.png',\n",
       " 'image_path': 'C:\\\\Users\\\\egese\\\\Desktop\\\\dataset\\\\val\\\\SAP\\\\8d281e0d909c4b9c842d967421edae11.png',\n",
       " 'classification': {'category': 'Excel-Tabelle',\n",
       "  'confidence': 0.7050408124923706,\n",
       "  'status': 'success'},\n",
       " 'reference_used': {'filename': 'b3444b458ad24811833429eb7c7dacb8.png',\n",
       "  'path': 'C:\\\\Users\\\\egese\\\\Desktop\\\\dataset\\\\mapped_train\\\\Excel-Tabelle\\\\b3444b458ad24811833429eb7c7dacb8.png',\n",
       "  'details': {'image_path': 'C:\\\\Users\\\\egese\\\\Desktop\\\\dataset\\\\mapped_train\\\\Excel-Tabelle\\\\b3444b458ad24811833429eb7c7dacb8.png',\n",
       "   'filename': 'b3444b458ad24811833429eb7c7dacb8.png',\n",
       "   'category': 'Excel-Tabelle',\n",
       "   'predicted_class': 'Excel-Tabelle',\n",
       "   'confidence': 0.9967940449714661,\n",
       "   'category_match': True,\n",
       "   'dimensions': '1920x1080',\n",
       "   'file_size_mb': 0.2,\n",
       "   'format': 'PNG',\n",
       "   'quality_score': 5,\n",
       "   'confidence_score': 9,\n",
       "   'total_score': 7.5,\n",
       "   'evaluation_date': '2025-06-22T20:32:25.434880'},\n",
       "  'category_path': 'Excel-Tabelle'},\n",
       " 'evaluation': {'struktur_qualitaet': {'hat_tabellenstruktur': True,\n",
       "   'spalten_anzahl': '4 (Referenz: 2)',\n",
       "   'zeilen_anzahl': '100+ (Referenz: 10)',\n",
       "   'hat_kopfzeile': True,\n",
       "   'score': 7},\n",
       "  'visueller_aufbau': {'farbschema': 'bunt (Referenz: blau)',\n",
       "   'gridlines_sichtbar': True,\n",
       "   'text_lesbarkeit': 'gut',\n",
       "   'score': 9},\n",
       "  'funktionale_elemente': {'formeln_vorhanden': False,\n",
       "   'summen_berechnung': True,\n",
       "   'diagramme_enthalten': False,\n",
       "   'filter_aktiviert': False,\n",
       "   'score': 5},\n",
       "  'sap_kontext': {'sap_interface_erkennbar': True,\n",
       "   'daten_typ': 'BW Query',\n",
       "   'business_kontext': 'Financial',\n",
       "   'score': 8},\n",
       "  'gesamt_score': 7.5,\n",
       "  'verbesserungsvorschlaege': ['Bessere Formatierung der Zahlen (z.B. Tausendertrennzeichen)',\n",
       "   'Konsistente Farbcodierung f√ºr bessere √úbersichtlichkeit',\n",
       "   'Hinzuf√ºgen von Diagrammen f√ºr visuelle Darstellung der Trends',\n",
       "   'Formeln einf√ºgen f√ºr dynamische Berechnungen',\n",
       "   'Filteroptionen aktivieren f√ºr bessere Datenanalyse'],\n",
       "  'bewertungs_zusammenfassung': {'gesamtpunktzahl': 75,\n",
       "   'note': 2.5,\n",
       "   'bestanden': True,\n",
       "   'hauptkritikpunkte': ['Fehlende Formeln und Filteroptionen',\n",
       "    'Zu viele Zeilen, was die √úbersichtlichkeit beeintr√§chtigt'],\n",
       "   'verbesserungsempfehlungen': ['Reduzieren der Zeilenanzahl durch Aggregation oder Filterung',\n",
       "    'Einbinden von dynamischen Formeln und Filtern',\n",
       "    'Hinzuf√ºgen von Diagrammen f√ºr bessere visuelle Darstellung']},\n",
       "  'qualitaetspruefung': {'is_evaluable': True,\n",
       "   'reason': 'Das Bild zeigt eine klare und lesbare Tabelle mit finanziellen Daten, die f√ºr eine Bewertung geeignet sind. Es gibt keine UI-Elemente, die den Inhalt st√∂ren, und die Qualit√§t ist ausreichend hoch.'},\n",
       "  'evaluation_status': 'evaluated'},\n",
       " 'timestamp': '2025-06-24T22:33:08.713424',\n",
       " 'status': 'success'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## üß™ Quick Test - Single Image Evaluation\n",
    "\n",
    "# Test the updated system with category-specific prompts\n",
    "def test_single_image_evaluation():\n",
    "    \"\"\"Test evaluation with a single image\"\"\"\n",
    "    if evaluator is None:\n",
    "        print(\"‚ùå Evaluator not initialized\")\n",
    "        return\n",
    "    \n",
    "    # Test with a sample image from val/SAP\n",
    "    test_images = [f for f in os.listdir(VAL_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    if not test_images:\n",
    "        print(\"‚ùå No test images found in val/SAP\")\n",
    "        return\n",
    "    \n",
    "    # Pick first available image\n",
    "    test_image = \"8d281e0d909c4b9c842d967421edae11.png\"\n",
    "    test_path = os.path.join(VAL_DIR, test_image)\n",
    "    \n",
    "    print(f\"üß™ Testing with: {test_image}\")\n",
    "    print(f\"üìÅ Path: {test_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluator.evaluate_student_submission(test_path, test_image)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüéØ EVALUATION RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        classification = result['classification']\n",
    "        print(f\"üìä CNN Classification: {classification['category']} ({classification['confidence']:.2%})\")\n",
    "        \n",
    "        reference = result['reference_used']\n",
    "        print(f\"üéØ Reference: {reference['filename']}\")\n",
    "        \n",
    "        evaluation = result['evaluation']\n",
    "        if 'error' not in evaluation:\n",
    "            # Check for category-specific results first\n",
    "            if 'bewertungs_zusammenfassung' in evaluation:  # New category-specific format\n",
    "                bewertung = evaluation['bewertungs_zusammenfassung']\n",
    "                score = bewertung.get('gesamtpunktzahl', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                passed = bewertung.get('bestanden', False)\n",
    "                feedback = f\"Category-specific evaluation for {classification['category']}\"\n",
    "                \n",
    "                print(f\"‚úÖ Final Score: {score}/100\")\n",
    "                print(f\"üìù Grade: {grade}\")\n",
    "                print(f\"üéì Passed: {'Yes' if passed else 'No'}\")\n",
    "                print(f\"üí¨ Feedback: {feedback}\")\n",
    "                \n",
    "                # Show category-specific analysis structure\n",
    "                print(f\"üè∑Ô∏è Category-Specific Analysis: {classification['category']}\")\n",
    "                \n",
    "                # Show main sections from category-specific analysis\n",
    "                category_sections = [k for k in evaluation.keys() if k not in ['bewertungs_zusammenfassung']]\n",
    "                if category_sections:\n",
    "                    print(f\"üìä Analysis Sections: {', '.join(category_sections[:4])}\")\n",
    "                \n",
    "                # Show criticisms and recommendations\n",
    "                kritik = bewertung.get('hauptkritikpunkte', [])\n",
    "                empfehlungen = bewertung.get('verbesserungsempfehlungen', [])\n",
    "                \n",
    "                if kritik:\n",
    "                    print(f\"‚ö†Ô∏è Main Issues: {len(kritik)} items\")\n",
    "                    for i, issue in enumerate(kritik[:2], 1):\n",
    "                        print(f\"   {i}. {issue}\")\n",
    "                        \n",
    "                if empfehlungen:\n",
    "                    print(f\"üí° Recommendations: {len(empfehlungen)} items\")\n",
    "                    for i, rec in enumerate(empfehlungen[:2], 1):\n",
    "                        print(f\"   {i}. {rec}\")\n",
    "                        \n",
    "            elif 'gesamtbewertung' in evaluation:  # Generic format fallback\n",
    "                bewertung = evaluation['gesamtbewertung']\n",
    "                score = bewertung.get('gesamtpunktzahl', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                passed = bewertung.get('bestanden', False)\n",
    "                feedback = bewertung.get('feedback', 'No feedback')\n",
    "                \n",
    "                print(f\"‚úÖ Final Score: {score}/100\")\n",
    "                print(f\"üìù Grade: {grade}\")\n",
    "                print(f\"üéì Passed: {'Yes' if passed else 'No'}\")\n",
    "                print(f\"üí¨ Feedback: {feedback}\")\n",
    "                \n",
    "                # Check for suggestions (both formats)\n",
    "                suggestions = evaluation.get('empfehlungen', evaluation.get('verbesserungsvorschlaege', []))\n",
    "                if suggestions:\n",
    "                    print(f\"üí° Suggestions: {len(suggestions)} items\")\n",
    "                    for i, suggestion in enumerate(suggestions[:3], 1):\n",
    "                        print(f\"   {i}. {suggestion}\")\n",
    "                        \n",
    "                # Show category-specific analysis\n",
    "                if 'kategorie_vergleich' in evaluation:\n",
    "                    kategorie_info = evaluation['kategorie_vergleich']\n",
    "                    print(f\"üè∑Ô∏è Category: {kategorie_info.get('kategorie', 'Unknown')}\")\n",
    "                    print(f\"üìä Category Match: {kategorie_info.get('student_erfuellt_kategorie', False)}\")\n",
    "                    \n",
    "                # Show detailed scores\n",
    "                if 'detailbewertung' in evaluation:\n",
    "                    detail_scores = evaluation['detailbewertung']\n",
    "                    print(\"üìà Detailed Scores:\")\n",
    "                    print(f\"   Struktur: {detail_scores.get('struktur_score', 0)}/100\")\n",
    "                    print(f\"   Technik: {detail_scores.get('technik_score', 0)}/100\") \n",
    "                    print(f\"   Vollst√§ndigkeit: {detail_scores.get('vollstaendigkeit_score', 0)}/100\")\n",
    "                    print(f\"   Korrektheit: {detail_scores.get('korrektheit_score', 0)}/100\")\n",
    "                    \n",
    "            elif 'gesamt_bewertung' in evaluation:  # Old format (with underscore)\n",
    "                bewertung = evaluation['gesamt_bewertung']\n",
    "                score = bewertung.get('gesamtpunkte', 0)\n",
    "                grade = bewertung.get('note', 5.0)\n",
    "                passed = bewertung.get('bestanden', False)\n",
    "                feedback = bewertung.get('feedback', 'No feedback')\n",
    "                \n",
    "                print(f\"‚úÖ Final Score: {score}/100\")\n",
    "                print(f\"üìù Grade: {grade}\")\n",
    "                print(f\"üéì Passed: {'Yes' if passed else 'No'}\")\n",
    "                print(f\"üí¨ Feedback: {feedback}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Unexpected response format\")\n",
    "                print(\"Raw evaluation keys:\", list(evaluation.keys()))\n",
    "                # Try to extract any available scores\n",
    "                if 'detailbewertung' in evaluation:\n",
    "                    detail_scores = evaluation['detailbewertung']\n",
    "                    avg_score = sum(detail_scores.values()) / len(detail_scores) if detail_scores else 0\n",
    "                    print(f\"üìä Average Score: {avg_score:.1f}/100\")\n",
    "        else:\n",
    "            print(f\"‚ùå Evaluation error: {evaluation['error']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Evaluation failed: {result.get('error', 'Unknown error')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Ready to test!\n",
    "print(\"üß™ Ready to test category-specific prompts!\")\n",
    "print(\"üí° Call test_single_image_evaluation() to run a test\")\n",
    "test_single_image_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
